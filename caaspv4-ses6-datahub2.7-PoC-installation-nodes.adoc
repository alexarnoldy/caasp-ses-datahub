#### Format will be to link SES6 PoC installation notes (as of yet not created), then link the CaaSPv4 PoC installation notes: https://github.com/alexarnoldy/caasp-ses-datahub/blob/master/caaspv4-installation-notes.adoc, then create Data Hub v2.7 PoC installation steps (which will include CaaSP integration) here

CAUTION: All changes to the CaaSPv4 and SES6 procedures will be made in the linked document. All changes to the Data Hub v2.7 procedures will be made here.

////
* May want to try installing v2.6 if 2.7 is delayed by too much
** Some CRI-O notes from this doc: https://access.redhat.com/articles/4324391
*** 4.2 Kaniko Image Builder

By default, Pipeline Modeler (vflow) pod uses Docker Daemon on the node, where it runs, to build container images before they are run. This was possible on OCP releases prior to 4.0. Since then, OCP uses CRI-O containter runtime.

To enable Pipeline Modeler to build images on recent OCP releases, it must be configured to use kaniko image builder. This is achieved by passing --enable-kaniko=yes parameter to the install.sh script during the manual installation. For the other installation methods, one can enable it by appending --enable-kaniko=yes to SLP_EXTRA_PARAMETERS (Additional Installation Parameters).
4.2.1. Registry requirements for the Kaniko Image Builder

The Kaniko Image Builder supports out-of-the-box only connections to secure image registries with a certificate signed by a trusted certificate authority.

In order to use an insecure image registry (e.g. the proposed external image registry) in combination with the builder, the registry must be whitelisted in Pipeline Modeler by marking it as insecure.

* cri-o cheat sheet: https://cheatsheet.dennyzhang.com/cheatsheet-crio-a4
////

////
Need to link in private container image registry deployment procedures here
Need to include copying over private registry TLS certs to Admin
////

.Temporary fix for rootless podman until kernel version 4.18 which supports overlayfs under normal users:
* Take note of all nodes that do not run this container correctly. 
* Type `exit` in each container that does offer a command prompt ("/ #" or "/ # ^[[32;5R")

----
cd ~/caaspv4-cluster
for NODE in `cat .all_nodes`; do echo $NODE; ssh $NODE podman run -i -t --rm docker.io/alpine sh; done
----

* Perform the follow steps on each node that did not run the container correctly:
----
for NODE in `cat .all_nodes`; do echo $NODE; ssh $NODE "sed -i '/driver/ s/overlay/vfs/' /home/sles/.config/containers/storage.conf"; done
for NODE in `cat .all_nodes`; do echo $NODE; ssh $NODE rm /home/sles/.local/share/containers/storage/libpod/bolt_state.db; done
----
* Re-run the loop at the beginning of this step to ensure each node can run the alpine linux pod correctly

.Add private container image registry TLS certificate to each node:
----
for NODE in `cat .all_nodes`; do echo $NODE; scp /etc/pki/trust/anchors/dhregistry.caaspv4.com-ca.crt $NODE:/tmp; done
for NODE in `cat .all_nodes`; do echo $NODE; ssh $NODE sudo mv /tmp/dhregistry.caaspv4.com-ca.crt /etc/pki/trust/anchors/; done
for NODE in `cat .all_nodes`; do echo $NODE; ssh $NODE sudo update-ca-certificates; done
----


.Add the private container image registry to the /etc/containers/registries.conf file:
----
for NODE in `cat .all_nodes`; do echo $NODE; ssh $NODE "sudo sed -i 's/\[\"docker.io/\[\"dhregistry.caaspv4.com\"\\, \"docker.io/' /etc/containers/registries.conf"; done
----

.Ensure each node can push and pull to the private container image registry:
----
for NODE in `cat .all_nodes`; do echo $NODE; ssh $NODE podman tag docker.io/alpine:latest dhregistry.caaspv4.com/alpine:latest; done
for NODE in `cat .all_nodes`; do echo $NODE; ssh $NODE podman login --username admin --password myp@ssw0rd dhregistry.caaspv4.com; done
for NODE in `cat .all_nodes`; do echo $NODE; ssh $NODE podman push dhregistry.caaspv4.com/alpine:latest; done
----

.Ensure all cluster nodes can reach the Internet

NOTE: The pod that executes the SAP Data Hub Pipeline Engine API server must be able to access the Internet while building the container images requested by pipeline operators

----
for NODE in `cat .all_nodes`; do echo $NODE; ssh $NODE ping -c 2 opensuse.org; done
----

* Add the data-hub configuration context to the ~/caaspv4-cluster/admin.conf file:

----
- context:
    cluster: caaspv4-cluster
    user: kubernetes-admin
    namespace: data-hub
  name: data-hub
----

.Create the data-hub namespace and use the data-hub configuration context:
----
kubectl create namespace data-hub
kubectl config use-context data-hub
kubectl config get-contexts
----


.Create the cluster-admin clusterRoleBinding for Tiller and initialize Helm:
----
kubectl create serviceaccount --namespace kube-system tiller
kubectl create clusterrolebinding tiller --clusterrole=cluster-admin --serviceaccount=kube-system:tiller
helm init --service-account tiller --upgrade
----

////
Testing removing port 5000 from everything
`kubectl create secret docker-registry dhregistry-secret -n data-hub --docker-server=dhregistry.example.com:5000 --docker-username=admin --docker-password='myp@ssw0rd' --docker-email=admin@example.com`
////

.Create the image pull secret for the private container image registry:
----
kubectl create secret docker-registry dhregistry-secret -n data-hub --docker-server=dhregistry.example.com --docker-username=admin --docker-password='myp@ssw0rd' --docker-email=admin@example.com
kubectl patch sa default -n data-hub -p '"imagePullSecrets": [{"name": "dhregistry-secret" }]'
----

.Update the pod security policy to allow pods access to worker node's root filesystem:
* `kubectl edit psp suse.caasp.psp.privileged`
** Search for allowedHostPaths
*** If it is not already set, add the following to the bottom of the file:
----
  allowedHostPaths:
  - pathPrefix: /
----

.Bind the cluster role to each Data Hub service account:
----
cat <<EOF> clusterrolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: suse:caasp:psp:priviliged:default
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: suse:caasp:psp:privileged
subjects:
- kind: ServiceAccount
  name: default
  namespace: DATAHUBNAMESPACE
- kind: ServiceAccount
  name: vora-vsystem-DATAHUBNAMESPACE
  namespace: DATAHUBNAMESPACE
- kind: ServiceAccount
  name: DATAHUBNAMESPACE-elasticsearch
  namespace: DATAHUBNAMESPACE
- kind: ServiceAccount
  name: DATAHUBNAMESPACE-fluentd
  namespace: DATAHUBNAMESPACE
- kind: ServiceAccount
  name: DATAHUBNAMESPACE-nodeexporter
  namespace: DATAHUBNAMESPACE
- kind: ServiceAccount
  name: vora-vflow-server
  namespace: DATAHUBNAMESPACE
EOF
----

* `export NAMESPACE=data-hub && sed -i "s/DATAHUBNAMESPACE/${NAMESPACE}/g"  clusterrolebinding.yaml && kubectl apply -f clusterrolebinding.yaml`

NOTE: Perform the following steps only for a Rook managed SES cluster.

.Update the sles user's ~/.bashrc file and connect to the Rook container:

----
cat <<EOF>> ~/.bashrc
ROOK=$(kubectl -n rook-ceph get pod -l app=rook-ceph-tools -o jsonpath='{.items[0].metadata.name}')
alias rook="kubectl -n rook-ceph exec -it $ROOK -- ceph â€œ
EOF
----
* `kubectl exec -it $ROOK -n rook-ceph -- bash`

NOTE: Perform the next set of commands from the SES Administrative Workstation for a standard SES installation or from inside the rook-ceph-tools pod for a Rook managed SES cluster.


TIP: Use the command `ceph -s | grep osds` to find the number of OSDS in your ceph cluster. Use that information with the ceph osd calculator (https://ceph.com/pgcalc/) to get the number of Placement Groups "Suggested PG Count" that will be appropriate for the pool (CephPGSize)

.Create the Data Hub SES (Ceph) storage pool and enable RBD support
----
export CephPool="data-hub-demo-pool" 
export CephPGSize="16"
ceph osd pool create ${CephPool} ${CephPGSize}
ceph osd pool ls
ceph osd pool application enable ${CephPool} rbd
ceph osd pool application get ${CephPool}
----

TIP: Use the command `ceph auth ls | grep data-hub-demo` to ensure the user hasn't already been created. If it has, skip this step and continue onto "Gather the keys for the SES admin and data-hub-demo users"

.Create the user that will manage the pool

----
export CephUser="data-hub-demo" 
export CephPool="data-hub-demo-pool"
sudo ceph auth get-or-create client.${CephUser} mon 'allow r' osd "allow class-read object_prefix rbd_children, allow rwx pool=${CephPool}" -o /etc/ceph/ceph.client.${CephUser}.keyring
----

.Gather the keys for the SES admin and data-hub-demo users
----
ceph auth ls  | egrep -A1 "data-hub-demo|admin"
----
* Example ouput:
----
client.admin
        key: AQCliWtcAAAAABAAMRgUejj5FCG/bvLBpmKDUw==
----

.Encode each of the keys (admin key used as an example):
----
echo -n "AQCliWtcAAAAABAAMRgUejj5FCG/bvLBpmKDUw==" | base64
----
* Example ouput:
`QVFDbGlXdGNBQUFBQUJBQU1SZ1Vlamo1RkNHL2J2TEJwbUtEVXc9PQ==`


NOTE: The next commands should be run on the Management Workstation 

.Create the ceph-admin-secret
* Set this variable with the base64 encoded admin key: `ADMIN_KEY=""`
** For example: ADMIN_KEY="QVFDbGlXdGNBQUFBQUJBQU1SZ1Vlamo1RkNHL2J2TEJwbUtEVXc9PQ=="
----
cat <<EOF> ceph-secret-admin.yaml
apiVersion: v1
kind: Secret
metadata:
  name: ceph-secret-admin
  namespace: data-hub
type: "kubernetes.io/rbd"
data:
  key: $ADMIN_KEY
EOF
----

* Set this variable with the base64 encoded data-hub-demo key: `DATA_HUB_KEY=""`
** For example: DATA_HUB_KEY="QVFEaVJNdGR6K3dYTlJBQUhhTmRqS1c1eTl5MUd2VWkyZjhnS2c9PQ=="
----
cat <<EOF> ceph-secret-data-hub-demo.yaml
apiVersion: v1
kind: Secret
metadata:
  name: ceph-secret-data-hub-demo
  namespace: data-hub
type: "kubernetes.io/rbd"
data:
  key: $DATA_HUB_KEY
EOF
----

.Apply the two Kubernetes secrets:
* `kubectl apply -n data-hub -f ceph-secret-admin.yaml`
* `kubectl apply -n data-hub -f ceph-secret-data-hub-demo.yaml`

.Create the SES6 storage class:

NOTE: This procedure does not yet include a method for gathering IP addresses for the Monitor Nodes for a Rook managed SES cluster. The information is kept in the rook-ceph-tools pod.

* Set this variable to the ssh credentials for the SES Administrative Workstation: `SSH_SES_ADMIN=""`
** For example: SSH_SES_ADMIN=root@ses-admin.stable.suse.lab
* Set the MONITORS variable to include the IP addresses and ports for the SES monitor nodes:
----
MONITORS=`ssh $SSH_SES_ADMIN "grep mon_host /etc/ceph/ceph.conf" | awk -F" = " '{print$2}' | sed 's/\, /:6789\,/g' | sed 's/$/:6789/'`
echo $MONITORS
----
** The output should be similar to: `172.29.147.41:6789,172.29.147.40:6789,172.29.147.42:6789`

* Create the SES6 storage class
----
cat <<EOF> ses-rbd-sc.yaml
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: ses-rbd-sc
  annotations:
     storageclass.beta.kubernetes.io/is-default-class: "true"
provisioner: kubernetes.io/rbd
parameters:
  monitors: $MONITORS
  adminId: admin
  adminSecretName: ceph-secret-admin
  adminSecretNamespace: data-hub
  pool: data-hub-demo-pool
  userId: data-hub-demo
  userSecretName: ceph-secret-data-hub-demo
EOF
----

* Apply the kubernetes storage class:
`kubectl apply -n data-hub -f ses-rbd-sc.yaml`
* Verify the SES6 storage class is the default:
`kubectl get storageclass`

.Create a test PVC and ensure it can be bound:
----
cat <<EOF> test-pvc.yaml
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: test-pvc
  namespace: data-hub
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi
EOF
----
* Apply the kubernetes PVC:
`kubectl apply -n data-hub -f test-pvc.yaml`
* In less than one minute, the PVC should show that is "Bound" to "VOLUME":
`kubectl get pvc`
* Delete the PVC after it has shown to be Bound:
`kubectl delete -n data-hub -f test-pvc.yaml`

* If any master or worker nodes have less than 32GB, it is recommended to reboot each, in turn, before starting the installation to ensure they have the maximume amount of available memory for the installation.

* Run the SAP Data Hub installation script:
----
./install.sh -e vora-cluster.components.dlog.replicationFactor="1" -e vora-cluster.components.dlog.standbyFactor="0" -e vora-context-deploy.secop.profile=notls  --enable-kaniko yes --image-pull-secret dhregistry-secret --pv-storage-class ses-rbd-sc --accept-license --namespace data-hub --registry dhregistry.example.com --skip-preflight-checks --enable-checkpoint-store no
----












// vim: set syntax=asciidoc:
