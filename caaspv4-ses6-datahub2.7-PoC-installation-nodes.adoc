#### Format will be to link SES6 PoC installation notes (as of yet not created), then link the CaaSPv4 PoC installation notes: https://github.com/alexarnoldy/caasp-ses-datahub/blob/master/caaspv4-installation-notes.adoc, then create Data Hub v2.7 PoC installation steps (which will include CaaSP integration) here

CAUTION: All changes to the CaaSPv4 and SES6 procedures will be made in the linked document. All changes to the Data Hub v2.7 procedures will be made here.

////
* May want to try installing v2.6 if 2.7 is delayed by too much
** Some CRI-O notes from this doc: https://access.redhat.com/articles/4324391
*** 4.2 Kaniko Image Builder

By default, Pipeline Modeler (vflow) pod uses Docker Daemon on the node, where it runs, to build container images before they are run. This was possible on OCP releases prior to 4.0. Since then, OCP uses CRI-O containter runtime.

To enable Pipeline Modeler to build images on recent OCP releases, it must be configured to use kaniko image builder. This is achieved by passing --enable-kaniko=yes parameter to the install.sh script during the manual installation. For the other installation methods, one can enable it by appending --enable-kaniko=yes to SLP_EXTRA_PARAMETERS (Additional Installation Parameters).
4.2.1. Registry requirements for the Kaniko Image Builder

The Kaniko Image Builder supports out-of-the-box only connections to secure image registries with a certificate signed by a trusted certificate authority.

In order to use an insecure image registry (e.g. the proposed external image registry) in combination with the builder, the registry must be whitelisted in Pipeline Modeler by marking it as insecure.

* cri-o cheat sheet: https://cheatsheet.dennyzhang.com/cheatsheet-crio-a4
////

////
Need to link in private container image registry deployment procedures here
Need to include copying over private registry TLS certs to Admin
////

.Temporary fix for rootless podman until kernel version 4.18 which supports overlayfs under normal users:
* Take note of all nodes that do not run this container correctly. 
* Type `exit` in each container that does offer a command prompt ("/ #" or "/ # ^[[32;5R")

----
cd ~/caaspv4-cluster
for NODE in `cat .all_nodes`; do echo $NODE; ssh $NODE podman run -i -t --rm docker.io/alpine sh; done
----

* Perform the follow steps on each node that did not run the container correctly:
----
for NODE in `cat .all_nodes`; do echo $NODE; ssh $NODE "sed -i '/driver/ s/overlay/vfs/' /home/sles/.config/containers/storage.conf"; done
for NODE in `cat .all_nodes`; do echo $NODE; ssh $NODE rm /home/sles/.local/share/containers/storage/libpod/bolt_state.db; done
----
* Re-run the loop at the beginning of this step to ensure each node can run the alpine linux pod correctly

.Add private container image registry TLS certificate to each node:
----
for NODE in `cat .all_nodes`; do echo $NODE; scp /etc/pki/trust/anchors/dhregistry.caaspv4.com-ca.crt $NODE:/tmp; done
for NODE in `cat .all_nodes`; do echo $NODE; ssh $NODE sudo mv /tmp/dhregistry.caaspv4.com-ca.crt /etc/pki/trust/anchors/; done
for NODE in `cat .all_nodes`; do echo $NODE; ssh $NODE sudo update-ca-certificates; done
----


.Add the private container image registry to the /etc/containers/registries.conf file:
----
for NODE in `cat .all_nodes`; do echo $NODE; ssh $NODE "sudo sed -i 's/\[\"docker.io/\[\"dhregistry.caaspv4.com\"\\, \"docker.io/' /etc/containers/registries.conf"; done
----

.Ensure each node can push and pull to the private container image registry:
----
for NODE in `cat .all_nodes`; do echo $NODE; ssh $NODE podman tag docker.io/alpine:latest dhregistry.caaspv4.com/alpine:latest; done
for NODE in `cat .all_nodes`; do echo $NODE; ssh $NODE podman login --username admin --password myp@ssw0rd dhregistry.caaspv4.com; done
for NODE in `cat .all_nodes`; do echo $NODE; ssh $NODE podman push dhregistry.caaspv4.com/alpine:latest; done
----

.Ensure all cluster nodes can reach the Internet

NOTE: The pod that executes the SAP Data Hub Pipeline Engine API server must be able to access the Internet while building the container images requested by pipeline operators

----
for NODE in `cat .all_nodes`; do echo $NODE; ssh $NODE ping -c 2 opensuse.org; done
----

* Add the data-hub configuration context to the ~/caaspv4-cluster/admin.conf file:

----
- context:
    cluster: caaspv4-cluster
    user: kubernetes-admin
    namespace: data-hub
  name: data-hub
----

.Create the data-hub namespace and use the data-hub configuration context:
----
kubectl create namespace data-hub
kubectl config use-context data-hub
kubectl config get-contexts
----


.Create the cluster-admin clusterRoleBinding for Tiller and initialize Helm:
----
kubectl create serviceaccount --namespace kube-system tiller
kubectl create clusterrolebinding tiller --clusterrole=cluster-admin --serviceaccount=kube-system:tiller
helm init --service-account tiller --upgrade
----

////
Testing removing port 5000 from everything
`kubectl create secret docker-registry dhregistry-secret -n data-hub --docker-server=dhregistry.example.com:5000 --docker-username=admin --docker-password='myp@ssw0rd' --docker-email=admin@example.com`
////

.Create the image pull secret for the private container image registry:
----
kubectl create secret docker-registry dhregistry-secret -n data-hub --docker-server=dhregistry.example.com --docker-username=admin --docker-password='myp@ssw0rd' --docker-email=admin@example.com
kubectl patch sa default -n data-hub -p '"imagePullSecrets": [{"name": "dhregistry-secret" }]'
----

kubectl edit psp suse.caasp.psp.privileged

* `vi clusterrolebinding.yaml`
`export NAMESPACE=data-hub && sed -i "s/DATAHUBNAMESPACE/${NAMESPACE}/g"  clusterrolebinding.yaml && kubectl apply -f clusterrolebinding.yaml`

.For Rook installations, must first connect to the Rook container that can manage the Ceph cluster
`kubectl exec -it $ROOK -n rook-ceph -- bash`

NOTE: The next command sets need to be run inside the rook-ceph-tools pod

.Create the data-hub-demo-pool
----
export CephPool="data-hub-demo-pool" CephPGSize="16"
ceph osd pool create ${CephPool} ${CephPGSize}
ceph osd pool ls
ceph osd pool application enable ${CephPool} rbd
ceph osd pool application get ${CephPool}
----

.Create the user that will manage the pool
`ceph auth ls | grep data-hub-demo`
`export CephUser="data-hub-demo" CephPool="data-hub-demo-pool"`
`ceph auth get-or-create client.${CephUser} mon 'allow r' osd "allow class-read object_prefix rbd_children, allow rwx pool=${CephPool}" -o /etc/ceph/ceph.client.${CephUser}.keyring`
`ceph auth ls  | egrep -A1 "data-hub-demo|admin"`
`echo -n "AQAqxbVdiW2yCRAAZSgJsA5FRlwU2V52J50y0w==" | base64`

NOTE: The next commands need to be run on the Management Workstation or Master Node

.Create the ceph-admin-secret
----
# vi ceph-secret-admin.yaml

apiVersion: v1
kind: Secret
metadata:
  name: ceph-secret-admin
  namespace: data-hub
type: "kubernetes.io/rbd"
data:
  key: QVFBcXhiVmRpVzJ5Q1JBQVpTZ0pzQTVGUmx3VTJWNTJKNTB5MHc9PQ==
----

----
# vi ceph-secret-data-hub-demo.yaml

apiVersion: v1
kind: Secret
metadata:
  name: ceph-secret-data-hub-demo
  namespace: data-hub
type: "kubernetes.io/rbd"
data:
  key: QVFDTHRMaGRvVEZvREJBQXRQL3Frc2hiUEhkdnYzMkdqWC9XNGc9PQ==
----

`kubectl apply -n data-hub -f ceph-secret-data-hub-demo.yaml`
`kubectl apply -n data-hub -f ceph-secret-admin.yaml`

Management Workstation or Master Node

.Create the ceph-admin-secret
----
# vi ceph-secret-admin.yaml

apiVersion: v1
kind: Secret
metadata:
  name: ceph-secret-admin
  namespace: data-hub
type: "kubernetes.io/rbd"
data:
  key: QVFBcXhiVmRpVzJ5Q1JBQVpTZ0pzQTVGUmx3VTJWNTJKNTB5MHc9PQ==
----

----
# vi ceph-secret-data-hub-demo.yaml

apiVersion: v1
kind: Secret
metadata:
  name: ceph-secret-data-hub-demo
  namespace: data-hub
type: "kubernetes.io/rbd"
data:
  key: QVFDTHRMaGRvVEZvREJBQXRQL3Frc2hiUEhkdnYzMkdqWC9XNGc9PQ==
----

`kubectl apply -n data-hub -f ceph-secret-data-hub-demo.yaml`
`kubectl apply -n data-hub -f ceph-secret-admin.yaml`

.Ensure the SES6 storage class is the default
* Vagrant includes a script, otherwise use k8s

.Test that a PVC can be created and bound:
----
# vi test-pvc.yaml

kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: test-pvc
  namespace: data-hub
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi
----

`kubectl apply -n data-hub -f test-pvc.yaml`
`kubectl get pvc`

.Delete the PVC after it has shown to be bound
`kubectl delete -n data-hub -f test-pvc.yaml`












// vim: set syntax=asciidoc:
