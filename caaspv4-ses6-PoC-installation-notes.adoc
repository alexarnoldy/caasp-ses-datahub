### CURRENT STAGE OF THIS DOCUMENT: Rapidly changing, highly incomplete, based on a four node configuration, though only three nodes are available at the time of this writing, likely brittle do to lots of automation based on this specific configuration.
#### Last updated: 2019.10.16, late AM PDT

#### Things to fix in this document:
* Need to link CaaSPv4 doc at the end
* Need to pull the nginx and some of the AutoYaST stuff out and put it into a separate doc that gets linked into this one
* Need to change the geeko user name to sles
* Need to change the order of the nodes from c -> a to a -> d
* Need to ensure three node to four node conversion (and be ready to easily support more than four nodes)

CAUTION: The procedure outlined in this document leverages AutoYaST. AutoYaST works slightly differently depending on if the server is configured to boot in (U)EFI mode or Legacy BIOS mode. This document attempts to provide the simplest procedures for both cases. 

#### This document is a strictly guided, step-by-step implementation of a specific configuration based on the SUSE Enterprise Storage 6 Deployment Guide: https://documentation.suse.com/ses/6/single-html/ses-deployment/#book-storage-deployment
* This specific configuration is based on five nodes:
** Administrative Workstation - VM or bare-metal host
** Four OSD Nodes - bare-metal hosts
** Three Monitors - collocated with the first three OSD Nodes
** One Rados (S3/Swift) Gateways (RGW) - collocated it the fourth OSD Node

.Overview of this installation procedure:
* Manually install the SLES15 SP1 on the SES6 Administrative Workstation
* Make an AutoYaST clone file of the Administrative Workstation
* Update the clone file for some basic differences between the architecture and configuration of the Administrative Workstation and the OSD Nodes
* AutoYaST install the first OSD Node 

TIP: If the differences between the Administrative Workstation and the OSD Nodes is too complex, it may be quicker and easier to manually install the first OSD Node, make and update an AutoYaST clone file of it, then use copies of that to install the remaining OSD Nodes.

* Update the (primarily networking) configuration of the first OSD Node
* Create an AutoYaST clone file of the first OSD Node
* Make and update copies of the first OSD Node's AutoYast clone file for the remaining nodes
* AutoYast install the remaining OSD Nodes
* Install and configure SES6

////
Note to author:
Format of this document will be to create SES6 PoC installation notes here, then link https://github.com/alexarnoldy/caasp-ses-datahub/blob/master/caaspv4-installation-notes.adoc
////

IMPORTANT: All changes to the SES6 procedures will be made here. All changes to the CaaSP4v procedures will be made in the linked document.  

CAUTION: This guide can be used as a reference while deploying different configurations, but deviating from the specified design will cause unpredicable results

NOTE: Though the SES6 cluster will be used primarily to support the CaaS Platform cluster, it will be installed as a stand-alone storage cluster. This will allow it support additional services beyond the CaaS Platform cluster

.Required preparation that is not yet covered
* Establish an IP schema for the cluster. In this document, the Administrative Workstation (aka ses-admin.stable.suse.lab) is 172.16.200.130, the four OSD/Monitor/RGW nodes are 172.16.200.131-134
* Populate DNS records that are available to the cluster nodes. In addition, the fully qualified hostname salt.stable.suse.lab to the Administrative Workstation

.Install the Administrative Workstation:
* Can be a VM, as long as it has access to the Internet and the cluster network
* 4 vCPU, 4096MB mem, 20GB qcow2 boot drive in default libvirt location
* Booting from ISO: Select "Installation" BUT DO NOT PRESS ENTER
* On "Boot Options:" line: `ifcfg=eth0=172.16.200.130/24,172.16.200.1,172.16.250.2,stable.suse.lab hostname=ses-admin.stable.suse.lab`
** Format of ifcfg=eth0 is <node IP address/cidr mask>,<default gateway>,<primary DNS server>,<search domain>
* Press Enter

IMPORTANT: Use the both the SLES 15 SP1 and SUSE Enterprise Storage 6 subscription keys to register all SES nodes, including the Administrative Workstation

* Use the SLES 15 SP1 subscription key first, as with a normal SUSE Linux Enterprise Server installation

IMPORTANT: This procedured currently does not support registering with an SMT/RMT server

* Enable the SUSE Enterprise Storage 6 and Containers Modules
** This should also enable the Basesystem Module 
* Provide the SUSE Enterprise Storage 6 subscription key
* Select the "Text Mode" System Role 

////
IMPORTANT: Use great care in selecting the system (boot) drive to ensure an OSD drive or write-caching drive isn't inadvertently used as the system drive. If this occurs, the node will need to be re-installed. If this error is propogated into the AutoYaST file that is used to install the remaining OSD nodes, all affect nodes will need to be re-installed.
////

* Select any partitioning scheme, but take the following into consideration:
** It is NOT RECOMMENDED to enable LVM 

IMPORTANT: LVM is not needed on SES cluster nodes and is incompatible with OSD drives. For that reason it is often safer to avoid LVM on all SES cluster nodes

** A separate Home Partition is not needed on any of the SES Nodes but can be configured, if desired
** It is highly recommended to deselect "Enlarge to RAM Size for Suspend"
* Create New User:
** User's Full Name and Username: geeko

IMPORTANT: It is recommended to use a secure password that will work with all elements of the PoC environment. Thus, a password should be selected that includes at least one upper case and one lower case letter, one number and at least one of the following three special characters: ! # $

* Select "Use this password for system administrator", but do not select Automatic Login 
* On the final "Installation Settings" screen:
** Under Security, disable the Firewall
* Install

.Finish preparing the Administrative Workstation:
* Enable passwordless sudo for the user geeko
** `sudo bash -c "echo 'geeko ALL=(ALL) NOPASSWD: ALL' >> /etc/sudoers.d/01geeko"`
* The easiest way to insert the Administrative Workstation's SSH key into the AutoYaST clone file is to add it to itself before creating the clone file
** `ssh-keygen`
*** Accept the defaults, though a passphrase can be configured here, if desired
** `cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys`
* Update the NTP client settings. 
** `sudo yast timezone`
*** `other Settings`
*** `Synchronize with NTP server`
**** This example simply accepts the default, public NTP service for the region 
***** Select a different NTP serer, if available 
*** `Synchronize now`
**** Synchronizing with the NTP service will take several seconds but should complete without error
*** `Run NTP as daemon`
*** `Save NTP Configuration`
*** `Accept`, then `OK`

.Create an AutoYaST clone file of the Management Workstation
* `sudo yast2 clone_system`
** Approve the installation of the autoyast2 package
* `mkdir ~/autoyast_templates`
* `sudo mv /root/autoinst.xml ~/autoyast_templates/`
* `sudo chown -R geeko:users ~/autoyast_templates/`
* `cp ~/autoyast_templates/autoinst.xml ~/autoyast_templates/ses-osd-c.xml`

* Setup Docker and the nginx webserver
** `sudo zypper -n install docker`
** `sudo systemctl start docker.service && sudo systemctl enable docker.service`
*** The output should show that a symlink was created
** `sudo usermod -aG docker geeko ; sudo su - geeko`
** Launch nginx webserver container: `docker run --name autoyast-nginx -v /home/geeko/autoyast_templates:/usr/share/nginx/html:ro -P -d nginx:latest`

IMPORTANT: This container WILL NOT automatically start after rebooting the Administrative Workstation. Use `docker start autoyast-nginx` to start it manually

* Find the network port used by the nginx container:
** `docker ps`
*** The port will listed under PORTS. For example, port 32768 would be indicated with: `0.0.0.0:32768->80/tcp`
* Set this variable to the nginx port: `NGINX_PORT=""`
* Test that the master autoyast file is available: `curl http://ses-admin.stable.suse.lab:$NGINX_PORT/ses-osd-c.xml`
** The output should display the entire ses-osd-c.xml file
*** To verify the output, compare the md5sum from each of the following two commands:
**** `md5sum autoyast_templates/ses-osd-c.xml`
**** `curl http://ses-admin.stable.suse.lab:$NGINX_PORT/ses-osd-c.xml | md5sum`

.Update the ses-osd-c.xml AutoYaST file with the correct hostname and IP address
* `sudo zypper -n install xmlstarlet`
* `cd ~/autoyast_templates/`
* Verify that getent returns the correct IP address and fully qualified hostname 
** `getent hosts ses-osd-c`

WARNING: If the getent command does not return the correct IP address and fully qualified hostanme, DO NOT run the following `xml ed` and `sed` commands

* Update hostname in the ses-osd-c.xml file: `xml ed -L -u "//_:networking/_:dns/_:hostname" -v ses-osd-c ses-osd-c.xml`

TIP: Use the command `grep ipaddr autoinst.xml` to verify the Administrative Workstation's IP address

** Set this variable to the Administrative Workstation's IP address (i.e. 172.16.200.130): `MANAGEMENTIP=""`
** `OSDIP=`getent hosts ses-osd-c | awk '{print$1}'`; sed -i "s/$MANAGEMENTIP/$OSDIP/" ses-osd-c.xml`

.Update the ses-osd-c.xml AutoYaST file based on architectural differences:
* If the OSD Nodes are bare-metal hosts, run this command: `xml ed -L -d "//_:services-manager/_:services/_:enable/_:service[text()='spice-vdagentd']"  ses-osd-c.xml`
* If the OSD Nodes boot in Legacy BIOS mode, run this command: `xml ed -L -u "//_:bootloader/_:loader_type" -v grub2 ses-osd-c.xml`
* If the OSD Nodes boot in (U)EFI mode, run this command: `xml ed -L -u "//_:bootloader/_:loader_type" -v grub2-efi ses-osd-c.xml`

IMPORTANT: In this procedure, AutoYaST will initially install the first OSD Node with the same network configuration as the Administrative Workstation. After the installation, the network will be changed to the OSD Node's installed configuration. That configuration will be captured and used to AutoYaST install the remaining nodes. If the Administrative Workstation's installed network configuration is not compatible with the OSD's network capabilities (i.e. the Administrative Workstation uses eth0 but eth0 on the OSD Node is not connect to the network), it is recommended to either edit the AutoYast file to create the OSD Node's installed network configuration, or install the first OSD Node manually. See the AutoYaST guide for more information: https://documentation.suse.com/sles/15-SP1/single-html/SLES-autoyast/#book-autoyast

.Update the correct boot drive for the OSD Node

CAUTION: The following steps assume that you know the size of the OSD Node's boot drive, the boot drive is a different size than all other drives on the OSD, and that all OSD Nodes are configured exactly the same. Configurations outside of these parameters are beyond the scope of this document.

* Remove the specified /dev/vda or /dev/sda element from the AutoYaST file: `xml ed -L -d "//_:partitioning/_:drive/_:device[text()='/dev/vda']" ses-osd-c.xml; xml ed -L -d "//_:partitioning/_:drive/_:device[text()='/dev/sda']" ses-osd-c.xml`

////
Doesn't need to be a conditional, as below
** If the Adminstrative Workstation is a VM, run this command: `xml ed -L -d "//_:partitioning/_:drive/_:device[text()='/dev/vda']" ses-osd-c.xml`
** If the Adminstrative Workstation is a bare-metal server, run this command: `xml ed -L -d "//_:partitioning/_:drive/_:device[text()='/dev/sda']" ses-osd-c.xml`
////

* Insert the minimum and maximum size search parameters for the boot drive:
** `vim ses-osd-c.xml`
** Add the following element right below:
  <partitioning config:type="list">
    <drive>
----
      <initialize config:type="boolean">true</initialize>
      <skip_list config:type="list">
        <listentry>
  	<!-- skip devices that are 100MB smaller or less than the desired boot drive -->
  	<skip_key>size_k</skip_key>
  	<skip_value>MINIMUM</skip_value>
  	<skip_if_less_than config:type="boolean">true</skip_if_less_than>
        </listentry>
        <listentry>
  	<!-- skip devices that are 100MB larger or more than the desired boot drive -->
  	<skip_key>size_k</skip_key>
  	<skip_value>MAXIMUM</skip_value>
  	<skip_if_more_than config:type="boolean">true</skip_if_more_than>
        </listentry>
      </skip_list>
----

** Update MINIMUM with a value that is about 100MB less, and MAXIMUM with a value that is about 100MB greater than the size of the boot drive
*** Both MINIMUM and MAXIMUM are expressed in kilobytes
**** For example, for a boot drive that is 550GB, set MINIMUM to 471859200 and MAXIMUM to 681574400
* Fill the root partition with all of the remaining space on the boot drive:
** To find the <partition> element for the root partition, search for the string `<mount>\/<`
*** Inside that <partition> element (normally below the <mount> subnode), change the value of the <size> subnode to `max`
**** For example, before the change it might look like: `<size>66561507328</size>` and after the change it will look like: `<size>max</size>`

.Add the software registration information

IMPORTANT: This procedured currently does not support registering with an SMT/RMT server

* Add the following element at the top of the file, right below <profile ... > 
----
  <suse_register>
    <do_registration config:type="boolean">true</do_registration>
    <email>MY_EMAIL_ADDRESS</email>
    <reg_code>MY_SLES_REGCODE</reg_code>
    <install_updates config:type="boolean">true</install_updates>
    <slp_discovery config:type="boolean">false</slp_discovery>
    <addons config:type="list">
      <addon>
        <!-- Basesystem Module -->
        <name>sle-module-basesystem</name>
        <version>15.1</version>
        <arch>x86_64</arch>
      </addon>
      <addon>
        <!-- Server Applications Module -->
        <!-- Depends on: Basesystem Module -->
        <name>sle-module-server-applications</name>
        <version>15.1</version>
        <arch>x86_64</arch>
      </addon>
      <addon>
        <!-- SUSE Enterprise Storage -->
        <!-- Depends on: Server Applications Module -->
        <name>ses</name>
        <version>6</version>
        <arch>x86_64</arch>
        <reg_code>MY_SES6_REGCODE</reg_code>
      </addon>
    </addons>
  </suse_register>
----
** Update MY_EMAIL_ADDRESS with the correct email address, plus MY_SLES_REGCODE and MY_SES6_REGCODE, with your registration codes

*** Add the following element directly above the <services-manager> element:

////
Need to consolidate this with the nginx server for the CaaS Platform nginx server. It is ses-admin.stable.suse.lab here and admin.caaspv4.com there
////

----
  <scripts>
    <post-scripts config:type="list">
      <script>
        <debug config:type="boolean">true</debug>
        <feedback config:type="boolean">false</feedback>
        <feedback_type/>
        <filename>autoyast_post_updates.sh</filename>
        <interpreter>shell</interpreter>
        <location><![CDATA[http://ses-admin.stable.suse.lab:32768/autoyast_post_updates.sh]]></location>
        <notification>Performing_Final_Updates</notification>
        <param-list config:type="list"/>
        <source><![CDATA[]]></source>
      </script>
    </post-scripts>
  </scripts>
----
** In the URL below, change the port number 32768 to the port number of your nginx container
* Save the file and exit vim

* Create the /home/geeko/autoyast_post_updates.sh file:
** `echo "echo 'geeko ALL=(ALL) NOPASSWD: ALL' >> /etc/sudoers.d/01geeko" > /home/geeko/autoyast_templates/autoyast_post_updates.sh`


.AutoYaST install the first OSD Node

IMPORTANT: The procedure for installing via AutoYaST is slightly different depending on if the target server is configured to boot in (U)EFI mode or Legacy BIOS mode. Be sure to verify the boot mode for a bare-metal server before continuing. Virtual Machines commonly boot in Legacy BIOS mode. For more information, see the SLES15 SP1 AutoYaST guide: https://documentation.suse.com/sles/15-SP1/single-html/SLES-autoyast/#book-autoyast

CAUTION: The steps below assume the OSD Node's eth0 has network access to the ses-admin node. If this is not the case, asjusted the "AutoYaST parameters" line below to specify a NIC on the OSD Node that has network access to the ses-admin node.

* Provide the SLES 15 SP1 DVD1 installer DVD or ISO to the BIOS of the OSD Node
* Start the OSD Node from DVD or ISO,  Select "Installation" at the DVD GRuB screen, but DO NOT PRESS ENTER
** If there is a "Boot Options" line near the bottom of the screen, provide the AutoYaST parameters, shown below. When ready, press Enter to boot the system.
** If there IS NOT a "Boot Options" line near the bottom of the screen, press the "e" key. Then provide the AutoYaST parameters shown below, at the end of the "linuxefi" line (Be sure to insert a space after "splash=silent"). When ready, press Ctrl+x to boot the system.
*** AutoYaST parameters: `autoyast=http://ses-admin.stable.suse.lab:<nginx port>/ses-osd-c.xml ifcfg=eth0=<IP of ses-osd-c>/24,<IP of gateway>,<IP of DNS server>,stable.suse.lab hostname=ses-osd-c.stable.suse.lab`

.After OSD Node completes installation, Adjust its networking to suit the environment 

NOTE: This document demonstrates the procedure for creating a bonded network from eth0
    and eth1, then assigning the node's IP address to that bond; however, your configuration may be different

* Perform the following steps from the OSD Node's console:

TIP: In yast, Tab will help you navigate through panes and options. Each option in yast will have a letter highlighted.
     Using "Alt" + that letter will directly open that option.

** `sudo yast lan`
** `(Use tab and the arrow keys to highlight eth0) -> Delete -> OK`
** `sudo yast lan`
** `Add -> Device Type -> Bond -> Next`
** `(Select Statically Assigned IP Address) -> IP Address -> (input the Master Node's IP address)`
** `(Adjust the Subnet Mask, if needed) -> Bonded Slaves -> Yes`
** `(Select both eth0 and eth1) -> Next`
** `Routing -> (Ensure the Device for Default IPv4 Gateway is "-") -> OK`
* Verify networking is functioning correctly:
** `ip a`
** `ping opensuse.com`


.Creating an AutoYaST clone of the OSD Node
** The following steps can be performed from the OSD Node's console or an SSH session
*** `sudo yast2 clone_system`
*** SCP the AutoYaST file to the Administrative Workstation. This will overwrite the original ses-osd-c.xml file. Make a copy first, if needed.
**** ` sudo scp /root/autoinst.xml ses-admin.stable.suse.lab:/home/geeko/autoyast_templates/ses-osd-c.xml `

.Create copies of the ses-osd-c.xml file for each of the remaining OSD Nodes

TIP: Perform the following steps from the Administrative Workstation as the geeko user

* `cd ~/autoyast_templates/`
* `for EACH in b a; do cp -p ses-osd-c.xml ses-osd-$EACH.xml; done`

.Edit each OSD Node's XML file to update the hostname and IP address

* Change the hostname value for each OSD Node
** `for EACH in b a; do xml ed -L -u "//_:networking/_:dns/_:hostname" -v ses-osd-$EACH ses-osd-$EACH.xml; done`
* Verify that getent returns the correct IP addresses and hostnames. If not, DO NOT run the subsequent xml ed for loop
** `for EACH in b a; do getent hosts ses-osd-$EACH; done`
* Change the ipaddr value for each OSD Node's external interface
** Set this variable to the ses-osd-c node's IP address: `OSD_C_IP=""`
** `for EACH in b a; do OSDIP=`getent hosts ses-osd-$EACH | awk '{print$1}'`; sed -i "s/$OSD_C_IP/$OSDIP/" ses-osd-$EACH.xml; done`


.Test that each OSD Node's XML file is available through the nginx webserver
* `docker ps`
* Set this variable to the port listed under PORTS: `NGINX_PORT=""`
* Test that each OSD Node autoyast file is available: `for EACH in b a; do curl http://ses-admin.stable.suse.lab:$NGINX_PORT/ses-osd-$EACH.xml | egrep "<hostname|ipaddr"; done`
** Verify each hostname and IP address is correct for each Worker Node

.AutoYaST install the next OSD Node

IMPORTANT: The procedure for installing via AutoYaST is slightly different depending on if the target server is configured to boot in (U)EFI mode or Legacy BIOS mode. To ensure a the boot mode for a bare-metal server, consult its BIOS before continuing. Virtual Machines commonly boot in Legacy BIOS mode. For more information, see the SLES15 SP1 AutoYaST guide: https://documentation.suse.com/sles/15-SP1/single-html/SLES-autoyast/#book-autoyast

TIP: It is recommended to fully install one OSD before continuing to the rest of the OSD Nodes.
     Once it is shown that one OSD Node can be fully installed with the AutoYaST configuration, multiple nodes can be installed simultaneously.

* Provide the SLES 15 SP1 DVD1 installer DVD or ISO to the VM or host BIOS
* Start the OSD Node from the DVD ISO,  Select "Installation" at DVD GRuB screen, but DO NOT PRESS ENTER
** If there is a "Boot Options" line near the bottom of the screen, provide the AutoYaST parameters, shown below. When ready, press Enter to boot the system.
** If there IS NOT a "Boot Options" line near the bottom of the screen, press the "e" key, then provide the AutoYaST parameters, shown below; at the end of the "linuxefi" line. When ready, press Ctrl+x to boot the system.
*** AutoYaST parameters: `autoyast=http://ses-admin.stable.suse.lab:<nginx port>/<osd node name>.xml ifcfg=eth0=<IP of osd node>/24,<IP of gateway>,<IP of DNS server>,stable.suse.lab hostname=<osd node name>.stable.suse.lab

.AutoYaST install the rest of the OSD Nodes
* Repeat the previous step, "AutoYaST install the rest of the Worker Nodes" for each of the remaining OSD Nodes

.After all OSD Nodes have completed the installation, ensure their basic configuration is correct
* Ensure each OSD Node's network configuration is correct:
** `for EACH in a b c; do echo ses-osd-$EACH; ssh ses-osd-$EACH ip a; echo "Press Enter for next system" && read NEXT; done`
* Ensure the time on all nodes, including the Administrative Workstation, are synchronized within two seconds of each other
** `for EACH in a b c; do echo ses-osd-$EACH; ssh ses-osd-$EACH date; done; echo Administrative Workstation; date`
* Update the /etc/hosts file so that localhost is associated with the public IP address:
* SSH to each OSD Node, or create the files on the admin, scp them then execute them through ssh:w

----
cat /tmp/hosts-file-update 
sudo cp -np /etc/hosts /etc/hosts.17.Oct.2019.16.42 && ls -1 /etc/hosts*
HOSTS=`getent hosts $(hostname)`
sed -e "/^127/ s/localhost//" -e "/^127/a  $HOSTS  $(hostname) localhost" /etc/hosts > /tmp/hosts
sudo mv /tmp/hosts /etc/hosts
geeko@ses-admin:~> cat <<EOF>/tmp/hosts-file-update
sudo cp -np /etc/hosts /etc/hosts.\`date +"%d.%b.%Y.%H.%M"\` && ls -1 /etc/hosts*
HOSTS=\`getent hosts \$(hostname)\`
sed -e "/^127/ s/localhost//" -e "/^127/a  \$HOSTS  \$(hostname) localhost" /etc/hosts > /tmp/hosts
sudo mv /tmp/hosts /etc/hosts
EOF
----

** `sudo cp -np /etc/hosts /etc/hosts.`date +"%d.%b.%Y.%H.%M"` && ls -1 /etc/hosts*`
sed -e '/^127/ s/localhost//' -e '/^127/a 172.16.200.132 ses-osd-c.stable.suse.lab ses-osd-c localhost' /etc/hosts > /tmp/hosts
sudo mv /tmp/host /etc/hosts

.Begin configuring the Administrative Workstation for deploying the SES6 cluster software

TIP: If the Administrative Workstation is a virtual machine, it is recommended to take a snapshot before continuing

TIP: Peform the following steps on the Administrative Workstation, which is also the Salt Master

* Install the salt-master and salt-minion packages on the Salt Master (ses-admin) 
** `sudo zypper -n install salt-master salt-minion`
* Ensure the salt-master salt-minion services are started and enabled
** `sudo systemctl start salt-master.service && sudo systemctl enable salt-master.service`
** `sudo systemctl start salt-minion.service && sudo systemctl enable salt-minion.service`
*** The output should show that a symlink was created

.Install, start and enable the salt-minion software on all OSD Nodes
* `for EACH in a b c; do echo ses-osd-$EACH; ssh ses-osd-$EACH sudo zypper -n install salt-minion; done`
* `for EACH in a b c; do echo ses-osd-$EACH; ssh ses-osd-$EACH sudo systemctl start salt-minion.service && sudo systemctl enable salt-minion.service salt-minion; done`
** The output from each OSD Node should show that a symlink was created


.Verify each Salt minion's fingerprint:
* View the fingerprints of all the OSD Node's salt minion keys:
** `for EACH in a b c; do echo ses-osd-$EACH; ssh ses-osd-$EACH sudo salt-call --local key.finger; done`
* List fingerprints of all unaccepted minion keys on the Salt master: `sudo salt-key -F`
** The fingerprints of all of the OSD Nodes should match what they reported

.Accept the the salt minion's keys, if the match what the OSD Nodes reported
* sudo salt-key --accept-all
* sudo salt-key --list-all

#### Prior to deploying SES6, manually zap all of the disks that will be used as OSD devices
.Determine the capacity of the OSD devices that will be used, as reported by lsblk
** `for EACH in a b c; do echo ses-osd-$EACH; ssh ses-osd-$EACH  lsblk | grep disk; done`

WARNING: This procedure assumes all OSD devices are the same capacity throughout the SES cluster. It is beyond the scope of this document to initialize the cluster with heterogeneous OSD or caching devices.

.Create a per OSD Node file containing the disks to be zapped
* Set this variable to the capacity of the devices to be zapped, exactly as reported by lsblk: `CAPACITY=""`
** For example: If lsblk reports `sdb       8:16   0   5.5T  0 disk`, the capacity to use would be "5.5T"
* Identify all devices to be zapped, on a per node basis: `for EACH in a b c; do cat /dev/null > ses-osd-$EACH-disks-to-zap; ssh ses-osd-$EACH  lsblk | grep $CAPACITY | grep disk | awk '{print$1}' >> ses-osd-$EACH-disks-to-zap; done`
* Validate that the disks to be zapped for each OSD Node are correct: `grep -v suse ses-osd-*-disks-to-zap`

NOTE: Additional devices, such as SATA/SAS/NVMe SSDs can be added to each file to zap all devices at the same time. This will also help in verifying all devices at the end of the procedure.

* Zap the beginning of each drive:
** `for EACH in a b c; do for DISK in `cat ses-osd-$EACH-disks-to-zap`; do echo ses-osd-$EACH:$DISK; ssh ses-osd-$EACH sudo dd if=/dev/zero of=/dev/$DISK bs=512 count=34 oflag=direct; done; done`
* Zap the end of each drive:
** `for EACH in a b c; do for DISK in `cat ses-osd-$EACH-disks-to-zap`; do export BLOCKDEV=`ssh ses-osd-$EACH sudo blockdev --getsz /dev/$DISK`; echo ses-osd-$EACH:$DISK; ssh ses-osd-$EACH  sudo dd if=/dev/zero of=/dev/$DISK bs=512 count=33 seek=$(($BLOCKDEV - 33)) oflag=direct; done; done`
* Return to the beginning of the step, changing the CAPACITY="" variable, to zap any additional devices that weren't zapped in the first effort.
* Verify that the disk labels have been removed from all of the devices
** `for EACH in a b c; do echo ses-osd-$EACH; for DISK in `cat ses-osd-$EACH-disks-to-zap`; do ssh ses-osd-$EACH sudo parted -s /dev/$DISK print free 1>/dev/null; done; done`


#### Begin deploying the SES6 cluster

IMPORTANT: The procedure generally follows the official SES6 Deployment Guide: https://documentation.suse.com/ses/6/single-html/ses-deployment/#book-storage-deployment. While the intention is to keep it current, no guarantee is given that it will keep pace with any changes in the official deployment guide. It is highly recommended to review the official documentation before continuing.

TIP: Peform the following steps on the Administrative Workstation, which is also the Salt Master

.Configure the Administrative Workstation for the role of Salt Master

* Install the deepsea package : `sudo zypper -n install deepsea`
* The contents of `/etc/salt/minion_id` must be included in what is returned by `sudo salt-key -L`
** If not, consult the deployment guide for remediation

.Run the prep and discovery Salt stages of the deployment
* Switch to the root user: `sudo -i`
* Verify that all of the Salt minions (OSD Nodes) are correctly targeted: `salt 'ses*' test.ping`
** The Administrative Workstation and all of the OSD Nodes respond with "True"

CAUTION: If the output includes "No minions matched the target." the /srv/pillar/ceph/deepsea_minions.sls will need to be updated before continuing. For more information see the "Matching the Minion Name" section of the SES6 Deployment Guide:https://documentation.suse.com/ses/6/single-html/ses-deployment/#ds-minion-targeting-name

* Run the prep Salt stage: `salt-run state.orch ceph.stage.0`
** This command can take several minutes to complete
* Run the discovery Salt stage: `salt-run state.orch ceph.stage.1`

.Create the policy.cfg and drive_groups.yml file
* Create the policy.cfg file: 
----
cat <<EOF> /srv/pillar/ceph/proposals/policy.cfg
cluster-ceph/cluster/*.sls
role-master/cluster/ses-admin*.sls
role-admin/cluster/ses-admin*.sls
role-prometheus/cluster/ses-admin*.sls
role-grafana/cluster/ses-admin*.sls
role-mon/cluster/ses-osd-[abc].sls
role-mgr/cluster/ses-osd-[abc].sls
role-rgw/cluster/ses-osd-c.sls
config/stack/default/global.yml
config/stack/default/ceph/cluster.yml
role-storage/cluster/ses-osd-*.sls
EOF
----

* Update the drive_groups.yml file to target the OSD Nodes:
** `mv /srv/salt/ceph/configuration/files/drive_groups.yml /srv/salt/ceph/configuration/files/drive_groups.yml.orig`
** `grep ^# /srv/salt/ceph/configuration/files/drive_groups.yml.orig > /srv/salt/ceph/configuration/files/drive_groups.yml`
----
cat <<EOF>> /srv/salt/ceph/configuration/files/drive_groups.yml
default:
  target: 'ses-osd-*'
  data_devices:
rotational: 1
  db_devices:
rotational: 0
EOF
----

////
** `sed -i 's/I\@roles\:storage/ses-osd-\*/' /srv/salt/ceph/configuration/files/drive_groups.yml
////

* Verify the capacity of the OSD devices and cache/db devices, as they will be seen by the ceph-volume command:
** `salt-run disks.details  | grep -A1 -B2 size`
*** Ensure the capacity of a devices listed as "rotation: 1" are the same and all devices listed as "rotational: 0" are the same

NOTE: Initializing non-uniform OSD Nodes is beyond the scope of this document. For more information on correcly parsing non-uniform OSD Nodes, see the Drive Groups section of the SES6 Deployment Guide: https://documentation.suse.com/ses/6/single-html/ses-deployment/#ds-drive-groups-specs

* If all rotational devices will be used as OSD devices, and all non-rotational devices will be used as cache/db devices; the drive_groups.yml file does not need to be updated
* If needed, update the /srv/salt/ceph/configuration/files/drive_groups.yml file to add size specification to the data devices and db_devices
** An expression of `size: ':1TB'` would limit the data_devices or db_devices to only those whose capacity is less than 1TB
** An expression of `size: '5TB:'` would limit the data_devices or db_devices to only those whose capacity is greater than 5TB
*** Therefore, the following code block would limit data_devices to rotational devices greater than 5TB and db_devices (aka cache/db devices) to non-rotational devices less than 1TB:
----
default:
  target: 'ses-osd-*'
  data_devices:
    rotational: 1
    size: '5TB:'
  db_devices:
    rotational: 0
    size: ':1TB'
----

.Run the configure and deploy Salt stages of the deployment
* Ensure the "/srv/pillar/ceph" directory is owned by the salt user and group: `chown -R salt:salt /srv/pillar/ceph`
* Run the configure Salt stage: `salt-run state.orch ceph.stage.2`
** This command can take several minutes to complete
* Run the deploy Salt stage: `salt-run state.orch ceph.stage.3`





when you get to accepting salt keys, if all the host names aren't showing up as fully qualified, we need to look at it and remediate, as this will break stuff after the fact. 

also, there is a bug in SLES15SP1 where if you create a bond and assign the fqdn hostname to the interface that it will lose it after the first reboot. you have to go back into yast2 and re-declare it, and then it sticks.






// vim: set syntax=asciidoc:
