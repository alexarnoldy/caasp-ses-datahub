### CURRENT STAGE OF THIS DOCUMENT: Occasionally changing, possibly incomplete, likely brittle do to lots of automation based on this specific configuration
#### Last updated: 2019.10.08, late AM PDT

#### This document is a strictly guided, step-by-step implementation of a specific configuration based on the CaaS Platform v4.0.0 Deployment Guide: https://documentation.suse.com/suse-caasp/4/single-html/caasp-deployment/
* This specific configuration is based on five nodes:
** Management Workstation - VM or bare-metal host
** One CaaS Platform Master Node - VM or bare-metal host
** Three CaaS Platform Worker Nodes - Must be same as Master Node

NOTE: Unlike CaaS Platform v3, the Management Workstation for CaaS Platform v4 is not tied in a 1:1 relationship with the CaaS Platform cluster.
      A Management Workstation can be used to deploy one or more CaaS Platform clusters and a CaaS Platform cluster can be managed from the 
      Management Workstation that deployed it and/or another appropriately configured Management Workstation.

CAUTION: This guide can be used as a reference while deploying different configurations, but deviating from these instructions will cause unpredicable results


.Required preparation that is not yet covered
* Establish an IP schema for the cluster. In this document, the Management Workstation (aka admin.caaspv4.com) is 172.16.241.200, the master and three worker nodes are 172.16.241.105-108
* Populate DNS records that are available to the cluster nodes

.Install the Management Workstation:

* Can be a VM, as long as it has access to the Internet and the cluster network
* 4 vCPU, 4096MB mem, 20GB qcow2 boot drive in default libvirt location
* Booting from ISO: Select "Installation" BUT DO NOT PRESS ENTER
* On "Boot Options:" line: `ifcfg=eth0=172.16.241.200/24,172.16.241.254,172.16.250.2,caaspv4.com hostname=admin.caaspv4.com`
** Format of ifcfg=eth0 is <node IP address/cidr mask>,<default gateway>,<primary DNS server>,<search domain>
* Press Enter

IMPORTANT: Use the CaaS Platform subscription key, not the SLES subscription key to register all CaaS Platform nodes, including the Management Workstation

* Use the CaaS Platform subscription key to register SLES 15 SP1
** Enable the following repositories:
*** SUSE CaaS Platform 4.0 
**** Should automatically enable Basesystem Module and Containers Module
*** Desktop Applications Module
**** Note: A desktop environment is not needed for this installation
           If you want a desktop environment on the Management Workstation, all of the CaaS Platform nodes will also have one
           There are ways around this, but they are outside the scope of this document
* For partitioning, select Guided Setup
** Keep defaults except deselect "Enable Swap"
*** Note: Swap will likely not be needed for the Management Workstation

CAUTION: If you choose to enable swap for the Management Workstation, you MUST remove it from the AutoYaST clone file for the CaaS Platform nodes. Swap is incompatible with Kubernetes

* Create New User:
** User's Full Name and Username: sles
** Select "Use this password for system administrator" and Automatic Login
* On the final "Installation Settings" screen:
** Under Security, disable the Firewall
* Install



.Finish preparing the Management workstation:
** `sudo bash -c "echo 'sles ALL=(ALL) NOPASSWD: ALL' >> /etc/sudoers"`
** `ssh-keygen`
** `cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys`
*** This is the eaisest way to insert the Management Workstation's SSH key into the AutoYaST clone file
** `sudo yast timezone`
*** `other Settings`
*** `Synchronize with NTP server`
*** `Synchronize now`
*** `Run NTP as daemon`
*** `Save NTP Configuration`
*** `Accept`, then `OK`

.Create an AutoYaST clone file of the Management Workstation
** `sudo yast2 clone_system`
*** Approve the installation of the AutoYaST2 package
** `mkdir autoyast_templates`
** `sudo mv /root/autoinst.xml ~/autoyast_templates/master.xml`
** `sudo chmod -R 777 ~/autoyast_templates/`

.Update the master.xml AutoYaST file with the correct hostname and IP address
* `cd autoyast_templates/`
* Verify that getent returns the correct IP address and hostname. If not, DO NOT run the subsequent `xml ed` and `sed` commands
** `getent hosts master`
* Update hostname in the master.xml file: xml ed -L -u "//_:networking/_:dns/_:hostname" -v master master.xml
** Set this variable to the Management Workstation's IP address (i.e. 172.16.241.105): MANAGEMENTIP=""
** `MASTERIP=`getent hosts master | awk '{print$1}'`; sed -i "s/$MANAGEMENTIP/$MASTERIP/" master.xml`

////
Manual way of updating hostname and IP address
*** `cd autoyast_templates/; vim master.xml`
**** Search for <\/hostname
***** Change hostname from admin to master
**** Search for `<ipaddr`
***** Change the IP address to that of the master. In this document it is 172.16.241.105
////
* Update the correct boot drive for the Master Node

CAUTION: The following steps assume that the first drive to be probed is the Master Node's boot drive. If this is not the case, edit the AutoYaST file manually to set the correct boot drive

** If the Master Node is a VM, run this command: `xml ed -L -u "//_:partitioning/_:drive/_:device"[1] -v "/dev/vda" master.xml`
** If the Master Node is a bare-metal server, run this command: `xml ed -L -u "//_:partitioning/_:drive/_:device"[1] -v "/dev/sda" master.xml`
* `cd autoyast_templates/; vim master.xml`
* If the Master Node is a VM, run this command: xml ed -L -d "//_:services-manager/_:services/_:enable/_:service[text()='spice-vdagentd']"  autoinst.xml
* Add the following element at the top, right below <profile ... > and update with correct email and reg code
----
  <suse_register>
    <do_registration config:type="boolean">true</do_registration>
    <email>tux@example.com</email>
    <reg_code>MY_SECRET_REGCODE</reg_code>
    <install_updates config:type="boolean">true</install_updates>
    <slp_discovery config:type="boolean">false</slp_discovery>
    <addons config:type="list">
      <addon>
        <!-- Containers Module -->
        <name>sle-module-containers</name>
        <version>15.1</version>
        <arch>x86_64</arch>
      </addon>
      <addon>
        <!-- SUSE CaaS Platform -->
        <!-- Depends on: Containers Module -->
        <name>caasp</name>
        <version>4.0</version>
        <arch>x86_64</arch>
        <reg_code>MY_SECRET_REGCODE</reg_code>
      </addon>
    </addons>
  </suse_register>
----

////
* Create the XML block to register CaaS Platform 4.0
----
cat <<EOF> reg_code.xml 
  <suse_register>
    <do_registration config:type="boolean">true</do_registration>
    <email>tux@example.com</email>
    <reg_code>MY_SECRET_REGCODE</reg_code>
    <install_updates config:type="boolean">true</install_updates>
    <slp_discovery config:type="boolean">false</slp_discovery>
    <addons config:type="list">
      <addon>
        <!-- Containers Module -->
        <name>sle-module-containers</name>
        <version>15.1</version>
        <arch>x86_64</arch>
      </addon>
      <addon>
        <!-- SUSE CaaS Platform -->
        <!-- Depends on: Containers Module -->
        <name>caasp</name>
        <version>4.0</version>
        <arch>x86_64</arch>
        <reg_code>MY_SECRET_REGCODE</reg_code>
      </addon>
    </addons>
  </suse_register>
EOF
----

////

*** Create the /home/sles/autoyast_post_updates.sh file
**** ` echo "echo 'sles ALL=(ALL) NOPASSWD: ALL' >> /etc/sudoers.d/01sles" >> /home/sles/autoyast_templates/autoyast_post_updates.sh `

*** Add the following element directly above the <services-manager> element:
*** In the URL below, change the port number 32768 to the port number of your nginx container

----
  <scripts>
    <post-scripts config:type="list">
      <script>
        <debug config:type="boolean">true</debug>
        <feedback config:type="boolean">false</feedback>
        <feedback_type/>
        <filename>autoyast_post_updates.sh</filename>
        <interpreter>shell</interpreter>
        <location><![CDATA[http://admin.caaspv4.com:32768/autoyast_post_updates.sh]]></location>
        <notification>Performing_Final_Updates</notification>
        <param-list config:type="list"/>
        <source><![CDATA[]]></source>
      </script>
    </post-scripts>
  </scripts>
----

** Setting up Docker and the nginx webserver
*** `sudo zypper -n in docker`
*** `sudo systemctl start docker.service && sudo systemctl enable docker.service`
*** `sudo usermod -aG docker sles ; sudo su - sles`
*** Launch nginx container: `docker run --name autoyast-nginx -v /home/sles/autoyast_templates:/usr/share/nginx/html:ro -P -d nginx:latest`
**** Note: This container will not automatically start after rebooting the Management Workstation. Use `docker start autoyast-nginx` to start it manually
*** `docker ps`
**** Set this variable to the port listed under PORTS (i.e. 32768): `NGINX_PORT=""`
*** Test that the master autoyast file is available: `curl http://admin.caaspv4.com:$NGINX_PORT/master.xml`

.AutoYaST install the Master Node
* Provide the SLES 15 SP1 DVD1 installer DVD or ISO to the VM or host BIOS
* Start the Master Node from DVD ISO,  Select "Installation" at DVD GRuB screen, but DO NOT PRESS ENTER
* On Boot Options line: `autoyast=http://admin.caaspv4.com:<nginx port>/master.xml ifcfg=eth0=<IP of master>/24,<IP of gateway>,<IP of DNS server>,caaspv4.com hostname=master.caaspv4.com`

.After Master Node completes installation, Adjust its networking to suit the environment 
* Note: This document shows the procdure for creating a bonded network from eth0
    and eth1, then assigning the node's IP address to that bond 
** Your configuration may be different
** VM CaaS Platform nodes will likely not need any network modifications
* Perform the following steps from the Master Node's conosle:

TIP: In yast, Tab will help you navigate through panes and options. Each option in yast will have a letter highlighted.
     Using "Alt" + that letter will directly open that option.

** sudo yast lan
** (Highlight eth0) -> Delete -> OK
** sudo yast lan
** Add -> Device Type -> Bond -> Next
** (Select Statically Assigned IP Address) -> IP Address -> (input the Master Node's IP address)
** (Adjust the Subnet Mask, if needed) -> Bonded Slaves -> Yes
** (Select both eth0 and eth1) -> Next
** Routing -> (Ensure the Device for Default IPv4 Gateway is -) -> OK
* Verify networking is functioning correctly:
** ip a
** ping google.com

.Ensure the Master Node does not have swap enabled. Swap is incompatible with Kubernetes
* `cat /proc/swaps`
** Should return a header line, but nothing else
* `grep swap /etc/fstab`
** Should return nothing
*** If swap is enabled, remote the swap line from the /etc/fstab file and reboot

.Add Master Node SSH key to its own authorized_keys file so it will be included in the AutoYaST clone file
* `ssh-keygen`
** Accept the defaults
* `cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys`

.Creating an AutoYaST clone of the Master Node
** The following steps can be performed from the Master Node's console or an SSH session
*** `sudo yast2 clone_system`
*** SCP the AutoYaST file to the Management Workstation. This will overwrite the original master.xml file. Make a copy first, if needed.
**** ` sudo scp /root/autoinst.xml admin.caaspv4.com:/home/sles/autoyast_templates/master.xml `

.Create copies of the master.xml file for each Worker Node

TIP: Perform the following steps from the Management Workstation as the sles user

* `cd ~/autoyast_templates/`
* `for EACH in 1 2 3; do cp -p master.xml worker$EACH.xml; done`

.Edit each Worker Node XML file to update the hostname and IP address
////
Note: Due to the "<profile xmlns=" default namespace declaration in the AutoYaST file, xmlstarlet selects and edits follow a different format:
To select the hostname: xml sel -t -m "//_:networking/_:dns" -v _:hostname FILENAME.xml
To update the hostname: xml ed -L -u "//_:networking/_:dns/_:hostname" -v <new hostname> FILENAME.xml
////

* Change the hostname value for each Worker Node
** `for EACH in 1 2 3; do xml ed -L -u "//_:networking/_:dns/_:hostname" -v worker$EACH worker$EACH.xml; done`
* Verify that getent returns the correct IP addresses and hostnames. If not, DO NOT run the subsequent xml ed for loop
** `for EACH in 1 2 3; do getent hosts worker$EACH; done`
* Change the ipaddr value for each Worker Node's external interface
** Set this variable to the Master Node's IP address: MASTERIP=""
** `for EACH in 1 2 3; do WORKERIP=`getent hosts worker$EACH | awk '{print$1}'`; sed -i "s/$MASTERIP/$WORKERIP/" worker$EACH.xml; done`

////
This was the manual way to update hostname and IP address
** `for EACH in 1 2 3; do vim worker$EACH.xml; done`
*** Search for <\/hostname
**** Change hostname from master to the correct Worker Node name
*** Search for <ipaddr
**** Change the IP address to that of the correct Worker Node
*** Use the command `:x` to save the file and move on the the next
////

.Test that each Worker Node XML file is available through the nginx webserver
* `docker ps`
* Set this variable to the port listed under PORTS: NGINX_PORT=""
* Test that each Worker Node autoyast file is available: `for EACH in 1 2 3; do curl http://admin.caaspv4.com:$NGINX_PORT/worker$EACH.xml | egrep "<hostname|ipaddr"; done`
** Verify each hostname and IP address is correct for each Worker Node

.AutoYaST install worker1
* 

TIP: It is recommended to fully install worker1 before continuing to the rest of the Worker Nodes.
     Once it is shown that worker1 can be fully installed with the AutoYaST configuration, multiple Worker Nodes can be installed simultaneously.

* Provide the SLES 15 SP1 DVD1 installer DVD or ISO to the VM or host BIOS
* Start the worker1 from DVD ISO,  Select "Installation" at DVD GRuB screen, but DO NOT PRESS ENTER
** On Boot Options line: `autoyast=http://admin.caaspv4.com:<nginx port>/<worker node name>.xml ifcfg=eth0=<IP of worker node>/24,<IP of gateway>,<IP of DNS server,caaspv4.com hostname=<worker node name>.caaspv4.com

.AutoYaST install the rest of the Worker Nodes
* Repeat the previous step, "AutoYast install worker1" for each of the remaining Worker Nodes

.Preparation for forming CaaS Platform cluster
* `eval "$(ssh-agent)"`
* `ssh-add /home/sles/.ssh/id_rsa`
* Verify passwordless SSH and sudo capabilities for the sles user on all nodes
** `for EACH in master worker1 worker2 worker3; do ssh $EACH sudo hostname; done`
*** Should return each hostname with no additional interaction required

.Ensure caasp, SLES, basesystem, and containers are all "Registered"
* `for EACH in master worker1 worker2 worker3; do echo $EACH; ssh $EACH sudo SUSEConnect -s | egrep --color "caasp|SLES|basesystem|containers|\"Registered\"" && echo"" && echo "Press Enter for next system" && read NEXT; done`

.Ensure swap is not enabled on any of the CaaS Platform hosts
* `for EACH in master worker1 worker2 worker3; do echo $EACH; ssh $EACH cat /proc/swaps; echo ""; done`
** Should return a header line for each node, but nothing else

.Bootstrap the cluster
* On the Management Workstation:
* `sudo zypper in -t pattern SUSE-CaaSP-Management`
* `skuba cluster init --control-plane master.caaspv4.com caaspv4-cluster`
** Note: Since we haven't created a load balancer, we are tying our control plane directly to the master node
* Ensure the SSH Agent is running and has the sles user's RSA key loaded
** `eval "$(ssh-agent)"`
** `ssh-add /home/sles/.ssh/id_rsa`
* `cd ~/caaspv4-cluster`
* `skuba node bootstrap --user sles --sudo --target master.caaspv4.com master`
** Note this command bootstraps the CaaS Platform cluster with master.caaspv4.com as the first (in this case, the only) master node. Internally, Kubernetes will assign this node the name "master"

.Join worker1 to the cluster
* Ensure the SSH Agent is running and has the sles user's RSA key loaded
** `eval "$(ssh-agent)"`
** `ssh-add /home/sles/.ssh/id_rsa`
* `cd ~/caaspv4-cluster`
* `WORKER_FQDN="worker1.caaspv4.com"`
* `WORKER="worker1"`
* `skuba node join --role worker --user sles --sudo --target $WORKER_FQDN $WORKER`

.Join each of the remain worker nodes to the cluster
* Repeat the previous step "Join worker1 to the cluster" for each of the remaining worker nodes, replacing worker1 with that node's name

.Verify the status of the cluster
* `cd ~/caaspv4-cluster/`
* `skuba cluster status`

.Enable the use of kubectl from the Management Workstation
* `echo export KUBECONFIG=/home/sles/caaspv4-cluster/admin.conf >> ~/.bashrc`
* `. ~/.bashrc` 
* `kubectl get nodes`

.Troubleshooting failed bootstrap
* ssh to master and `sudo less /var/log/messages` 
* Search for kub
* Follow the progression of the skuba command and kubeadm
** Generally skuba will install the packages, then launch kubeadm
** kubeadm will set up the K8s components
** If the failure occurs after kubeadm takes over try to replicate the failure:
*** scp kubeadm-init.conf from the cluster directory (caaspv4-cluster in this doc) to /tmp on the master node
*** Run the `kubeadm init` command that is in /var/log/messages
*** kubeadm should give reasonably actionable error messages


////
THIS WORK IS BASED ON PREVIOUS, FAILED ATTEMPTS. DO NOT USE!!!!!!
Install master node:
Deploy Deployment host O/S: Set IP on Grub line, enable repos: CaaSPv4, Containers, Package Hub
* Can enable SLES subscription with the CaaS Platform product key
* Disable Firewall
* Must have the same user across all nodes. Recommend use sles

.After installation complete:

* echo "sles ALL=(ALL) NOPASSWD: ALL" >> /etc/sudoers
* Add the ssh key from the sles user on the Management Workstation to the authorized_keys file
* Verify proper subscriptions:
** sudo SUSEConnect -s
*** SLES must be registered before continuing
** sudo SUSEConnect -p sle-module-containers/15.1/x86_64
** sudo SUSEConnect -p caasp/4.0/x86_64 -r <CAASP_PRODUCT_KEY>
* zypper update
* zypper in cri-o
* zypper -n in autoyast
* yast2 clone_system
** Note the underscore, not dash
* Output file is /root/autoinst.xml
* Need to update the autoinst.xml file with:
<ntp-client>
<suse_register>
<addon>

Need to take note of: The default AutoYaST file provides examples for a disabled 
root user and a sles user with authorized key SSH access.

cp -p autoinst.xml worker1.xml
vi worker1.xml
* Change 105 (the IP of the base node) to 106 for <ipaddr>
* Change <hostname> from master to worker1
* scp to deployment host: scp worker1.xml admin@deployer.caaspv4.com:/home/admin/autoyast_templates/worker1.xml

.On the Management Workstation:
* Create the user sles
* (as root) echo "sles ALL=(ALL) NOPASSWD: ALL" >> /etc/sudoers
* Verify proper subscriptions:
** sudo SUSEConnect -s
*** SLES must be registered before continuing
** sudo SUSEConnect -p sle-module-containers/15.1/x86_64
** sudo SUSEConnect -p caasp/4.0/x86_64 -r <CAASP_PRODUCT_KEY>
* Set up docker
Launch nginx container: docker run --name mynginx  -v /home/admin/autoyast_templates:/usr/share/nginx/html:ro -P -d nginx:latest
* Take note of the network port assigned to nginx
Test from master: curl http://deployer.caaspv4.com:<nginx port>/worker1.xml 

.Install worker hosts with AutoYaST:
* Start worker1 host from DVD ISO,  Select "Installation" at DVD GRuB screen, but DO NOT PRESS ENTER
* On Installation line: `autoyast=http://deployer.caaspv4.com:<nginx port>/worker1.xml ifcfg=eth0=<IP of worker1>/24,<IP of gateway>,<IP of DNS server,<search domain> hostname=worker1.caaspv4.com
* Repeat for worker2 and worker3

.Notes for skuba installation:

* Need a single SSH key and ssh-agent enabled:
** As the deployment user (sles in the deployment guide): 
*** Ensure it has an id_rsa key in ~/.ssh/
**** If not: ssh-keygen
***** Accept the defaults
* Start SSH Agent: eval "$(ssh-agent)"
* Check to see if it imported the local user's default key: ssh-add -l
** If not: ssh-add /home/sles/.ssh/id_rsa.pub


* Install skuba tools: sudo zypper in -t pattern SUSE-CaaSP-Management

* Make sure you are the user sles 
skuba cluster init --control-plane master.caaspv4.com caaspv4-cluster
cd caaspv4-cluster/


skuba node bootstrap --user sles --sudo --target master.caaspv4.com master
////





// vim: set syntax=asciidoc:
