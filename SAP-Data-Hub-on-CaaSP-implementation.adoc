This guide is based on a very specific design. The design is intended for Proof of Concept, training and other non-production work. Various aspects of this design can be altered, however configuring *below* the minimum resource allocations for the cluster nodes greatly increases the risk of difficulty troubleshooting errors and failures, as well as the inability to satisfactorily complete the installation. 

.The success of the CaaSP and Data Hub installation relies highly on the diligence of the prepration work. Some important details that must be accomplished before beginning the installation are:
////
* Configuration of a SES storage cluster that provides RBD and Rados Gateway services
////
* Configuration of a SES storage cluster that provides RBD services
** A minimum of 2TB of usable space
** The Rados Block Device service
*** Use https://ceph.com/pgcalc/ to get the commands to get the appropriate PG numbers for the pools
* DNS service configured with resolution for physical nodes as well as all CaaSP nodes
** There should be a dedicated virtual IP address that resolves to master.example.com
* Configuration of a TLS enabled Portus container registry (currently via docker-compose, running on the jumphost)
** In this document the FQDN for the Portus registry is dhregistry.example.com
* Configuration of a CaaSPv3 cluster with the following minimums:
** Four physical hosts, each provisioned with 32 logical CPUs (16 hyper-threaded cores), 64GB of memory, and 128GB of locally attached disk space

** One jumphost VM, provisioned with access to 4 logical CPUs, 8GB of memory, and 80GB of locally attached disk space
** One Admin VM node, provisioned with access to 16 logical CPUs, 16GB of memory, and 80GB of locally attached disk space
** Three master VM nodes, each provisioned with access to 16 logical CPUs, 24GB of memory, and 80GB of locally attached disk space
** Four worker VM nodes, each provisioned with access to 16 logical CPUs, 32GB of memory, and 256GB of locallly attached disk space

TIP: It is recommended that the master and worker nodes be spread across the available physical nodes with one of each on each physical node. The Admin node and jumphost would then be run on the node that doesn't have a master node.

TIP: The virtual machine's locally attached disk space can be stored in SES RBD images. In this case, it is recommended that the disk files for the virtual machines assigned to each physical node be stored on a single RBD image. This would result in four RBD images created in the SES cluster, one for each physical node.

.It is recommended that the following suggestions be observed when installing the CaaSP cluster:
* Provide network configuration for each CaaSP node, including the Admin node, through the Installation bootloader. Carefully typing in the information into the bootloader allows for the most consistent network configurations.
* Leverage AutoYaST when configuring the CaaSP cluster nodes for the easiest, most consistent cluster node installation.


== Outline of steps:
////
These first steps are omitted until they can be tested and documented
. Install physical nodes with SLES15 SP1
. Install physical nodes with SES 6
////
. <<anchor-10>>
. <<anchor-20>>
. <<anchor-30>>
. <<anchor-35>>
. <<anchor-40>>
.. Set up primary route to public router
.. Secondary route to NAT router to the storage VLAN
. <<anchor-50>>
.. Set primary route to public router
.. After installation, scp /etc/sysconfig/network/routes from admin to all nodes, then reboot all nodes
.. Verify that all nodes can ping google.com, admin.example.com, 172.16.200.130
.. Configure keepalived on the three master nodes:
. <<anchor-55>>
. <<anchor-60>>
. <<anchor-65>>
. <<anchor-70>>
. <<anchor-80>>

[[anchor-10]]
==== Create an SES RBD image for each physical host

* Set up the host to work with the SES cluster:

NOTE: Run the following four commands on each physical host

----
sudo zypper -n in ceph-common
export SESAdminNode="172.16.200.130"
sudo bash -c "scp ${SESAdminNode}:/etc/ceph/ceph.client.admin.keyring /etc/ceph"
sudo bash -c "scp ${SESAdminNode}:/etc/ceph/ceph.conf /etc/ceph"
----

* Create the user that will manage the infrastructure pool:

TIP: Use the command `ceph auth ls | grep data-hub-infra` to ensure the user hasn't already been created. If it has, skip this step and continue on to "Import the keyring for the data-hub-infra user"

NOTE: Run the following two commands from each physical host:

----
export CephUser="data-hub-infra" CephPool="data-hub-infra-pool"
ceph auth get-or-create client.${CephUser} mon 'allow r' osd \'allow class-read object_prefix rbd_children, allow rwx pool=${CephPool}\' -o ceph.client.${CephUser}.keyring
----

* Import the keyring for the data-hub-infra user

NOTE: Run the following two commands from each physical host:

----
export CephUser="data-hub-infra" SESAdminNode="172.16.200.130"
sudo bash -c "scp ${SESAdminNode}:/etc/ceph/ceph.client.${CephUser}.keyring /etc/ceph"
----


* Create the SES (Ceph) storage pool for the physical hosts and enable RBD support 

TIP: Use the ceph osd calculator (https://ceph.com/pgcalc/) to get the number of Placement Groups for the pool (CephPGSize)

NOTE: Run the following five commands from just one physical host

----
export CephPool="data-hub-infra-pool" CephPGSize="16"
ceph osd pool create ${CephPool} ${CephPGSize}
ceph osd pool ls
ceph osd pool application enable ${CephPool} rbd
ceph osd pool application get ${CephPool}
----

* Create an RBD image for each host

NOTE: Run the following two commands on each physical host

----
export Host=`hostname` CephPool="data-hub-infra-pool"
rbd create -s 512G ${CephPool}/${Host}
----

[[anchor-20]]
==== Map and mount the RBD images on each physical host

* Mount the new RBD image to the host and cause it to remount during system boot

NOTE: Run the following eight commands on each physical host 

----
export Host=`hostname` CephPool="data-hub-infra-pool" CephUser="data-hub-infra"
sudo bash -c "echo ${CephPool}/${Host}'     'id=${CephUser},keyring=/etc/ceph/ceph.client.${CephUser}.keyring" >> /etc/ceph/rbdmap"
sudo bash -c "echo /dev/rbd/${CephPool}/${Host}'     '/mnt/${CephPool}/${Host}'     'ext4'     'noauto'     '0'  '0 >> /etc/fstab"
sudo mkdir -p /mnt/${CephPool}/${Host}
sudo rbd map ${CephPool}/${Host}
sudo mkfs.ext4 /dev/rbd/${CephPool}/${Host} 
sudo mount /mnt/${CephPool}/${Host} 
sudo systemctl start rbdmap.service && sudo systemctl enable rbdmap.service
----

[[anchor-30]]
==== Create VMs on RBD images

[[anchor-35]]
==== Create Portus secure private Docker registry
* Follow this work-in-progress guide to create a Portus registry using docker-compose: https://github.com/alexarnoldy/caasp-ses-datahub/blob/master/portus-docker-compose

[[anchor-40]]
==== Install Admin node through console

[[anchor-50]]
==== Install CaaSP cluster nodes with AutoYaST
.Configure keepalived on the three master nodes:
* Create the /opt/docker-keepalived/keepalived.conf file on master1:
----
vrrp_instance VI_1 {
    state MASTER                
    interface eth0              
    virtual_router_id 40        
    priority 103
    track_interface {
        eth0                    
    }
    virtual_ipaddress {
        172.16.200.57           # replace this with your virtual IP
    }
    nopreempt
}
----

* Create the /opt/docker-keepalived/keepalived.conf file on master2:
----
vrrp_instance VI_1 {
    state BACKUP                
    interface eth0              
    virtual_router_id 40        
    priority 102
    track_interface {
        eth0                    
    }
    virtual_ipaddress {
        172.16.200.57           # replace this with your virtual IP
    }
    nopreempt
}
----

////
VAR MASTERVIP=172.16.200.57
////
* Create the /opt/docker-keepalived/keepalived.conf file on master3:
----
vrrp_instance VI_1 {
    state BACKUP                
    interface eth0              
    virtual_router_id 40        
    priority 101
    track_interface {
        eth0                    
    }
    virtual_ipaddress {
        172.16.200.57           # replace this with your virtual IP
    }
    nopreempt
}
----

* Run this command on each master node:
----
docker run -it -d --restart=always --net=host --privileged -v /opt/docker-keepalived/keepalived.conf:/etc/keepalived/keepalived.conf     --name haproxy-keepalived     susecaasp/caasp_keepalived:latest
----

* Test pinging the virtual IP address while rebooting the master nodes to verify proper keepalived operation
** The VIP will prefer to run first on master1, then master2, and then only on master3 if the first two are not available

[[anchor-55]]
==== Form the CaaSP cluster:
* Through the Velum GUI, accept all nodes 
* Assign the three master nodes for the role of "Master" and the four worker nodes for the roller of "Worker"
* Form cluster
** Use master.example.com as "External Kubernetes API FQDN"
** Use admin.example.com as "External Dashboard FQDN"
** Bootstrap the cluster
* After the cluster has formed, wait for Admin node to discover software updates then update Admin node (via Velum), followed by the rest of the cluster
** CMD: watch kubectl get nodes -o wide
*** When updated anything on the cluster, this is a good way to view progress of the update and determine if one node is having problems

////
After deploying Portus, need to add it to Velum with its certificate (Need to include steps to deploy Portus)
////
* Add the Portus private container registry to Velum:
** Name: dhregistry.example.com
** URL: https://dhregistry.example.com:5000
** Certificate: (Copy/Paste in from the secrets directory in Portus)

////
May need to scp the /etc/ntp.conf file to all k8s nodes and then start && enable ntpd.service on them
Will include in the doc after next opportunity to test
////

////
Will add this back in for CaaSPv4 when there is no Admin node
* Can get the kubeconfig for the Admin node from Velum, or:
** Copy the .kube/config file from the Admin node to the jumphost 
*** Change https://api.infra.caasp.local:6443 to https://master.example.com:6443
** Copy all of the certificate files in .kube/config from the Admin node to the jumphost
////
* Add the following to the .kube/config file:
----
- context:
    cluster: default-cluster
    user: cluster-admin
    namespace: data-hub
  name: data-hub
----
* Create the data-hub namespace and use the data-hub configuration context:
----
kubectl create namespace data-hub
kubectl config use-context data-hub
kubectl config get-contexts
----


[[anchor-60]]
==== Apply Data Hub specific modifications to CaaSP cluster
////
.After nodes are all updated, start preparing the cluster for the Data Hub installation:
////

* From the Admin node, check the /etc/docker/daemon.json files:
----
docker exec -it $(docker ps -q -f name="salt-master") salt -P 'roles:(admin|kube-master|kube-minion)' cmd.run "cat /etc/docker/daemon.json"
----

* Each cluster node (except for the admin) should have:
** Copy the file to any nodes that need it, then restart docker.service on that node
----
    {
      "registries": [
        {
          "Prefix": "https://registry.suse.com"
        },
        {
          "Prefix": "https://dhregistry.example.com:5000"
        }
      ],
      "iptables":false,
      "log-level": "warn",
      "log-driver": "json-file",
      "log-opts": {
        "max-size": "10m",
        "max-file": "5"
      }
    }

----

* The pod that executes the SAP Data Hub Pipeline Engine API server must be able to access the Internet while building the container images requested by pipeline operators
* Ensure all cluster nodes can reach the Internet
----
docker exec -it $(docker ps -q -f name="salt-master") salt -P 'roles:(kube-master|kube-minion)' cmd.run "ping -c 2 google.com"
----

* Create the cluster-admin clusterRoleBinding for Tiller and initialize Helm:
----
kubectl create clusterrolebinding tiller --clusterrole=cluster-admin --serviceaccount=kube-system:tiller
helm init --client-only --service-account tiller
----

* Add imagePullSecret to default service account in the data-hub namespace:

////
VAR REGISTRY=dhregistry
VAR DOMAINNAME=example.com
VAR DATAHUBNAMESPACE=data-hub
VAR PASSWORD=myp@ssw0rd
////

kubectl create secret docker-registry dhregistry-secret -n data-hub --docker-server=dhregistry.example.com:5000 --docker-username=admin --docker-password='myp@ssw0rd' --docker-email=admin@example.com
kubectl patch sa default -n data-hub -p '"imagePullSecrets": [{"name": "dhregistry-secret" }]'

////
From the jumphost: 
	Add to /etc/ceph/rbdmap:
	caasp01-aba-vms/data-hub        id=admin,keyring=/etc/ceph/ceph.client.admin.keyring
	Add to /etc/fstab:
	/dev/rbd/caasp01-aba-vms/data-hub       /mnt/caasp01-aba-vms/data-hub   ext4    noauto  0  0
		Save to /dev/rbd/caasp01-aba-vms/data-hub
////


----
kubectl edit psp suse.caasp.psp.privileged
----
.Search for allowedHostPaths 
.If allowedHostPaths is not alrady in the configuration, add the following below, and at the same indentation, as “volumes:”
----
  allowedHostPaths:
  - pathPrefix: /
----

* Create clusterrolebinding.yaml:

----
# vi clusterrolebinding.yaml

apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: suse:caasp:psp:priviliged:default
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: suse:caasp:psp:privileged
subjects:
- kind: ServiceAccount
  name: default
  namespace: DATAHUBNAMESPACE
- kind: ServiceAccount
  name: vora-vsystem-DATAHUBNAMESPACE
  namespace: DATAHUBNAMESPACE
- kind: ServiceAccount
  name: DATAHUBNAMESPACE-elasticsearch
  namespace: DATAHUBNAMESPACE
- kind: ServiceAccount
  name: DATAHUBNAMESPACE-fluentd
  namespace: DATAHUBNAMESPACE
- kind: ServiceAccount
  name: DATAHUBNAMESPACE-nodeexporter
  namespace: DATAHUBNAMESPACE
- kind: ServiceAccount
  name: vora-vflow-server
  namespace: DATAHUBNAMESPACE
----

----
export NAMESPACE=data-hub && sed -i "s/DATAHUBNAMESPACE/${NAMESPACE}/g"  clusterrolebinding.yaml && kubectl apply -f clusterrolebinding.yaml
----

[[anchor-65]]
==== Integrate SUSE Enterprise Storage RBD service into CaaSP

////
The following constitutes a lot of thrashing around to find the magic combination. Likely won't be of much value but keeping it around anyway
### Doesn't seem to work. Possibly due to the special character in the password
#docker exec -it $(docker ps -q -f name="salt-master") salt -P 'roles:(kube-master|kube-minion)' cmd.run "docker login dhregistry.example.com:5000 -u admin -p 'myp@ssw0rd'"

### Likely isn't needed since having the imagePullSecret working should be enough
#admin:~ # docker exec -it $(docker ps -q -f name="salt-master") salt -P 'roles:(kube-master|kube-minion)' cmd.run "hostname && docker pull nginx:latest && docker tag nginx:latest dhregistry.example.com:5000/nginx:latest && docker push dhregistry.example.com:5000/nginx:latest && docker pull dhregistry.example.com:5000/nginx:latest"




Create Ceph RBD pool and prepare CaaSP cluster to use it:

######
# Don't need to copy the files into the CaaSP cluster
######
#admin:~ # scp 172.16.200.130:/etc/ceph/* /etc/ceph
#Then, copy them from the CaaSP Admin node to the rest of the CaaSP cluster:
#admin:~ # for EE in 1 2 3 4; do scp /etc/ceph/* master$EE:/etc/ceph/; done
#admin:~ # for EE in 1 2 3 4; do scp /etc/ceph/* worker$EE:/etc/ceph/; done

#Verify all nodes can communicate with the CaaSP cluster:
#docker exec -it $(docker ps -q -f name="salt-master") salt -P 'roles:(admin|kube-master|kube-minion)' cmd.run "ceph -s"
////

////
VAR SESADMINNODE=172.16.200.130
////

* From the CaaSP Admin node:
----
scp 172.16.200.130:/etc/ceph/* /etc/ceph
----

////
VAR CEPHPOOL=data-hub-demo-pool
VAR CEPHPGSIZE=16
////

* Create the Data Hub SES (Ceph) storage pool and enable RBD support 

TIP: Use the ceph osd calculator (https://ceph.com/pgcalc/) to get the number of Placement Groups for the pool (CephPGSize)

----
export CephPool="data-hub-demo-pool" CephPGSize="16"
ceph osd pool create ${CephPool} ${CephPGSize}
ceph osd pool ls
ceph osd pool application enable ${CephPool} rbd
ceph osd pool application get ${CephPool}
----

////
VAR CEPHUSER=demo-hub-demo
////

* Create the user that will manage the pool 

TIP: Use the command `ceph auth ls | grep data-hub-demo` to ensure the user hasn't already been created. If it has, skip this step and continue on to "Gather the keys for the SES admin and data-hub-demo users"

----
export CephUser="data-hub-demo" CephPool="data-hub-demo-pool"
ceph auth get-or-create client.${CephUser} mon 'allow r' osd \'allow class-read object_prefix rbd_children, allow rwx pool=${CephPool}\' -o ceph.client.${CephUser}.keyring
----

* Gather the keys for the SES admin and data-hub-demo users
----
ceph auth ls  | egrep -A1 "data-hub-demo|admin"
----

* Encode each of the keys (admin key used as an example):
----
echo -n "AQCliWtcAAAAABAAMRgUejj5FCG/bvLBpmKDUw==" | base64
----
.Example ouput: 
QVFDbGlXdGNBQUFBQUJBQU1SZ1Vlamo1RkNHL2J2TEJwbUtEVXc9PQ==

* Create the Ceph admin user and data-hub-demo user secrets (use the base64 encoded keys you calculated above):
----
# vi ceph-secret-admin.yaml

apiVersion: v1
kind: Secret
metadata:
  name: ceph-secret-admin
  namespace: data-hub
type: "kubernetes.io/rbd"
data:
  key: QVFDbGlXdGNBQUFBQUJBQU1SZ1Vlamo1RkNHL2J2TEJwbUtEVXc9PQ==
----

----
# vi ceph-secret-data-hub-demo.yaml

apiVersion: v1
kind: Secret
metadata:
  name: ceph-secret-data-hub-demo
  namespace: data-hub
type: "kubernetes.io/rbd"
data:
  key: QVFDUU12WmN4VjV2RXhBQUVoekU5MWt3YmlHNmF0dzVPYUU0WUE9PQ==
----

* Apply the secrets:
----
# kubectl apply -n data-hub -f ceph-secret-data-hub-demo.yaml
# kubectl apply -n data-hub -f ceph-secret-admin.yaml
----


////
VAR CEPHMONITORS=172.16.200.132:6789,172.16.200.133:6789,172.16.200.134:6789
////

* Create the Storage Class and make it default:
----
# vi ses-rbd-sc.yaml

kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: ses-rbd-sc
  annotations:
     storageclass.beta.kubernetes.io/is-default-class: "true"
provisioner: kubernetes.io/rbd
parameters:
  monitors: 172.16.200.132:6789,172.16.200.133:6789,172.16.200.134:6789
  adminId: admin
  adminSecretName: ceph-secret-admin
  adminSecretNamespace: data-hub
  pool: data-hub-demo-pool
  userId: data-hub-demo
  userSecretName: ceph-secret-data-hub-demo
----

* Apply the Storage Class:
----
# kubectl apply -n data-hub -f ses-rbd-sc.yaml
# kubectl patch storageclass ses-rbd-sc -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"true"}}}'
# kubectl get storageclass
----
.Should show only one storage class and it is listed as (default)

* Test that a PVC can be created and bound:
----
# vi test-pvc.yaml

kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: test-pvc
  namespace: data-hub
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 100Gi
----

----
# kubectl apply -n data-hub -f test-pvc.yaml 
# kubectl get pvc
----

* After five to ten seconds, should show the PVC is bound
----
# kubectl delete -n data-hub -f test-pvc.yaml 
----


[[anchor-70]]
==== Install Data Hub

* Download the SAP Data Hub software from https://launchpad.support.sap.com/ and save it to the Admin node

* If any master or worker nodes have less than 32GB, it is recommended to reboot each, in turn, before starting the installation to ensure they have the maximume amount of available memory for the installation.

* Run the SAP Data Hub installation script:
----
./install.sh -e vora-cluster.components.dlog.replicationFactor="1" -e vora-cluster.components.dlog.standbyFactor="0" -e vora-context-deploy.secop.profile=notls  --image-pull-secret dhregistry-secret --pv-storage-class ses-rbd-sc --accept-license --namespace data-hub --registry dhregistry.example.com:5000 --skip-preflight-checks --enable-checkpoint-store no
----

////
	Add: --skip-preflight-checks if fails on helm version
	Use master.example.com as external Subject Alternative Name endpoint
////

* After installation completes, it will provide important information for accessing Data Hub. I.e.:
----
############ Ports for external connectivity ############
# vora-tx-coordinator-ext/tc port:                  31450
# vora-tx-coordinator-ext/hana-wire port:           32692
# vora-textanalysis/textanalysis port:              32196
# vsystem/vsystem port:                             31273
#########################################################

#########################################################
# System Tenant created:    "system"
# System Tenant User:       "system"
# Initial Tenant created:   "default"
# Initial Tenant User:      "suse"
# User for tx-coordinator:  "default\suse"
#########################################################
----

NOTE: Take note of the "vsystem/vsystem port:" number. This will be the port needed to reach the Data Hub UI

TIP: Use the command `kubectl get svc -n data-hub | grep  "vsystem "` to find the vsystem port number after the installation.

[[anchor-80]]
==== Apply Portus private container registry root certificate to Data Hub

* Import the Portus root CA into Data Hub:
** The root CA needs to be in .pem format (which is the same format but with a different suffix as .crt). It must be available on the system that is running the web browser used to access Data Hub.

////
VAR DATAHUBUSERNAME=suse
VAR DATAHUBUSERPASSWORD=myp@ssw0rd
////

* The SAP Data Hub Launchpad will be available at https://master.example.com:31273
** Log into the default Tenant as user suse and the password provided during installation.
** Select Connection Management -> Import, select certificate file and select Open




////
Will add this back in when CaaSPv4 is released
Jumphost (the Installation host) must have kubectl and helm installed. Both can be taken from the SUSE-CaaSP-3.0-Pool repository. Take info from Admin node to add the repo to the jumphost.

sudo zypper in kubernetes-client
sudo zypper in helm
////

////
Likely won't be needed
Test all nodes can pull from the private registry:
admin:~ # docker pull nginx:latest
admin:~ # docker tag nginx:latest dhregistry.example.com:5000/nginx:latest
admin:~ # docker login dhregistry.example.com:5000
admin:~ # docker push dhregistry.example.com:5000/nginx:latest
admin:~ # kubectl run nginx-test --image=dhregistry.example.com:5000/nginx --replicas=3
////




////
If a node seems to be having problems, try draining it: kubectl drain <node> --delete-local-data --ignore-daemonsets
If the pods restart correctly, uncordon the node: kubectl uncordon <node>
////

////
Not needed for this first round
Launch SAP HANA Express Docker container:
 
Host or VM must have lots of memory available (First deploy consumped about  9GB )

Add the following to /etc/sysctl.conf:
## HANA Express settings:
fs.file-max=20000000
fs.aio-max-nr=262144
vm.memory_failure_early_kill=1
vm.max_map_count=135217728
net.ipv4.ip_local_port_range=40000 60999

Must be logged into docker.io from system: docker login

Create /data/HANAExpress/passwd.json file:
{
  "master_password" : "myp@ssw0rd"
}

sudo chown -R 12000:79 /data/HANAExpress
sudo chmod 600  /data/HANAExpress/passwd.json

docker pull store/saplabs/hanaexpress:2.00.036.00.20190223.1

sudo docker run -d -p 39013:39013 -p 39017:39017 -p 39041-39045:39041-39045 -p 1128-1129:1128-1129 -p 59013-59014:59013-59014 -v /data/HANAExpress:/hana/mounts --ulimit nofile=1048576:1048576 --sysctl kernel.shmmax=1073741824 --sysctl net.ipv4.ip_local_port_range='40000 60999' --sysctl kernel.shmmni=524288 --sysctl kernel.shmall=8388608 --name HXE store/saplabs/hanaexpress:2.00.036.00.20190223.1 --passwords-url file:///hana/mounts/passwd.json --agree-to-sap-license
////


////
Start fidling with Ironic
Jason Douglas, Mike Latimer - 
////

////
##### Need to test pulling   dhregistry.example.com:5000/com.sap.hana.container/base-opensuse42.3-amd64   on nodes with smaller boot drives
////

// vim: set syntax=asciidoc:
