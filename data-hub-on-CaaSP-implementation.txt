This design is based on a very specific design. This design is intended for Proof of Concept, training and other non-production work. Various aspects of this design can be altered, however configuring below the minimum resource allocations for the cluster nodes greatly increases the risk of experiencing difficult to troubleshooting errors and failures, and the inability to satisfactorily complete the installation. 

The success of the CaaSP and Data Hub installation relies highly on the diligence of the prepration work. Some important details that must be accomplished are:
	* Configuration of a SES storage cluster that provides RBD service
		* A minimum of 2TB of usable space
		* The Rados Block Device service
	* Configuration of a TLS enabled Portus container registry (currently via docker-compose, running on the jumphost)
	* Configuration of a CaaSPv3 cluster with the following minimums:
		* Four physical hosts, each provisioned with 32 logical CPUs, 64GB of memory, and 128GB of locally attached disk space

		* One jumphost VM, provisioned with access to 4 logical CPUs, 8GB of memory, and 80GB of locally attached disk space
		* One admin VM node, provisioned with access to 16 logical CPUs, 16GB of memory, and 80GB of locally attached disk space
		* Three master VM nodes, each provisioned with access to 16 logical CPUs, 24GB of memory, and 80GB of locally attached disk space
		* Four worker VM nodes, each provisioned with access to 16 logical CPUs, 32GB of memory, and 256GB of locallly attached disk space
	* NOTE: It is recommended that the master and worker nodes be spread across the available physical nodes with one of each on each physical node. The admin node and jumphost would then be run on the node that doesn't have a master node.
	* NOTE: The virtual machine's locally attached disk space can be stored in SES RBD images. In this case, it is recommended that the disk files for the virtual machines assigned to each physical node be stored on a single RBD image. This would result in four RBD images created in the SES cluster, one for each physical node.

It is recommended that the following suggestions be observed when installing the CaaSP cluster:
	* Do not input the hostname of a node when assigning the node IP address. Only input the node name and domain name while providing the DNS information


Rough outline of steps:

Prep physical nodes for SES
Create an RBD image per host
Map and mount RBD images
Create VMs on RBD images
Install Admin node through console
	Set up primary route to public router
	Secondary route to NAT router to the storage VLAN

Install cluster nodes with autoyast
	Set primary route to public router
	After installation, scp /etc/sysconfig/network/routes from admin to all nodes, then reboot all nodes

Verify that all nodes can ping google.com, admin.aba.com, 172.16.200.130

ssh master1 mkdir -p /opt/docker-keepalived/
Create /opt/docker-keepalived/keepalived.conf on master1:
vrrp_instance VI_1 {
    state MASTER                # keepalived state
    interface eth0              # replace this with your interface
    virtual_router_id 40        
    priority 103
    track_interface {
        eth0                    # replace this with your interface
    }
    virtual_ipaddress {
        172.16.240.52           # replace this with your virtual IP
    }
    nopreempt
}

ssh master2 mkdir -p /opt/docker-keepalived/
Create /opt/docker-keepalived/keepalived.conf on master2:
vrrp_instance VI_1 {
    state BACKUP                # keepalived state
    interface eth0              # replace this with your interface
    virtual_router_id 40        
    priority 102
    track_interface {
        eth0                    # replace this with your interface
    }
    virtual_ipaddress {
        172.16.240.52           # replace this with your virtual IP
    }
    nopreempt
}

ssh master3 mkdir -p /opt/docker-keepalived/
Create /opt/docker-keepalived/keepalived.conf on master3:
vrrp_instance VI_1 {
    state BACKUP                # keepalived state
    interface eth0              # replace this with your interface
    virtual_router_id 40        
    priority 101
    track_interface {
        eth0                    # replace this with your interface
    }
    virtual_ipaddress {
        172.16.240.52           # replace this with your virtual IP
    }
    nopreempt
}

docker run -it -d --restart=always --net=host --privileged -v /opt/docker-keepalived/keepalived.conf:/etc/keepalived/keepalived.conf     --name haproxy-keepalived     pelin/haproxy-keepalived


Accept all nodes and form cluster. 
	Use master.aba.com as API endpoint
	Use admin.aba.com as cluster management endpoint

Wait for admin node to pickup new updates, then update admin and then rest of the cluster
	CMD: watch kubectl get nodes -o wide
		Best way to view progress of the update and determine if one node is causing problems


After nodes are all updated, start preparing the cluster for Data Hub:

Check the /etc/docker/daemon.json files before updating:
docker exec -it $(docker ps -q -f name="salt-master") salt -P 'roles:(admin|kube-master|kube-minion)' cmd.run "cat /etc/docker/daemon.json"

Each node (except for the admin) should have:
    {
      "registries": [
        {
          "Prefix": "https://registry.suse.com"
        },
        {
          "Prefix": "https://dhregistry.aba.com:5000"
        }
      ],
      "iptables":false,
      "log-level": "warn",
      "log-driver": "json-file",
      "log-opts": {
        "max-size": "10m",
        "max-file": "5"
      }
    }

If not, need to copy that file to each node and restart docker


The pod that executes the SAP Data Hub Pipeline Engine API server must be able to access the internet while building the container images requested by pipeline operators:

docker exec -it $(docker ps -q -f name="salt-master") salt -P 'roles:(kube-master|kube-minion)' cmd.run "ping -c 2 google.com"


Jumphost (the Installation host) must have kubectl and helm installed. Both can be taken from the SUSE-CaaSP-3.0-Pool repository. Take info from admin node to add the repo to the jumphost.

sudo zypper in kubernetes-client
sudo zypper in helm

Copy the .kube/config file from the admin node to the jumphost. Change https://api.infra.caasp.local:6443 to https://master.aba.com:6443
Copy all of the certificate files under /etc from admin node to jumphost
Add the following to the .kube/config file:
- context:
    cluster: default-cluster
    user: cluster-admin
    namespace: data-hub
  name: data-hub

Before you can start deployment, you must initialize Helm, the Kubernetes package manager, and provide it with the respective roles and permissions within the Kubernetes cluster.

kubectl create clusterrolebinding tiller --clusterrole=cluster-admin --serviceaccount=kube-system:tiller
helm init --client-only --service-account tiller

Create the data-hub namespace and use the data-hub configuration context:
kubectl create namespace data-hub
kubectl config use-context data-hub
kubectl config get-contexts


kubectl edit psp suse.caasp.psp.privileged
Search for allowedHostPaths first to ensure it isn’t already set elsewhere in the file
Add the following below and at the same indentation as “volumes:”

  allowedHostPaths:
  - pathPrefix: /

Create  clusterrolebinding.yaml:

apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: suse:caasp:psp:priviliged:default
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: suse:caasp:psp:privileged
subjects:
- kind: ServiceAccount
  name: default
  namespace: XXX
- kind: ServiceAccount
  name: vora-vsystem-XXX
  namespace: XXX
- kind: ServiceAccount
  name: XXX-elasticsearch
  namespace: XXX
- kind: ServiceAccount
  name: XXX-fluentd
  namespace: XXX
- kind: ServiceAccount
  name: XXX-nodeexporter
  namespace: XXX
- kind: ServiceAccount
  name: vora-vflow-server
  namespace: XXX


export NAMESPACE=data-hub && sed -i "s/XXX/${NAMESPACE}/g"  clusterrolebinding.yaml && kubectl apply -f clusterrolebinding.yaml

Copy the /etc/ceph files from the SES admin node to the CaaSP admin node:
admin:~ # scp 172.16.200.130:/etc/ceph/* /etc/ceph
Then, copy them from the CaaSP admin node to the rest of the CaaSP cluster:
admin:~ # for EE in 1 2 3 4; do scp /etc/ceph/* master$EE:/etc/ceph/; done
admin:~ # for EE in 1 2 3 4; do scp /etc/ceph/* worker$EE:/etc/ceph/; done

Verify all nodes can communicate with the CaaSP cluster:
docker exec -it $(docker ps -q -f name="salt-master") salt -P 'roles:(admin|kube-master|kube-minion)' cmd.run "ceph -s"


Set up Ceph secrets and default storage class in K8s:
Get the keys for data hub and admin users:
admin:~ # ceph auth ls  | egrep -A1 "caasp01|admin"

Encode keys (admin key as an example):
admin:~ # echo -n "AQCliWtcAAAAABAAMRgUejj5FCG/bvLBpmKDUw==" | base64
QVFDbGlXdGNBQUFBQUJBQU1SZ1Vlamo1RkNHL2J2TEJwbUtEVXc9PQ==

Create secrets:

admin@jumphost:~/data-hub-build> vi ceph-secret-admin.yaml
apiVersion: v1
kind: Secret
metadata:
  name: ceph-secret-admin
type: "kubernetes.io/rbd"
data:
  key: QVFDbGlXdGNBQUFBQUJBQU1SZ1Vlamo1RkNHL2J2TEJwbUtEVXc9PQ==

admin@jumphost:~/data-hub-build> vi ceph-secret-caasp01-aba.yaml
apiVersion: v1
kind: Secret
metadata:
  name: ceph-secret-caasp01-aba
type: "kubernetes.io/rbd"
data:
  key: QVFCT2JKdGN1OGFnTmhBQVkvb2RUWUFvZ3Q5T2g1WFptdDJDSEE9PQ==

admin@jumphost:~/data-hub-build> kubectl apply -n data-hub -f ceph-secret-caasp01-aba.yaml
admin@jumphost:~/data-hub-build> kubectl apply -n data-hub -f ceph-secret-admin.yaml


Create storage class and make it default:

admin@jumphost:~/data-hub-build> vi ses-rbd-sc.yaml
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: ses-rbd-sc
provisioner: kubernetes.io/rbd
parameters:
  monitors: 172.16.200.132:6789,172.16.200.133:6789,172.16.200.134:6789
  adminId: admin
  adminSecretName: ceph-secret-admin
  adminSecretNamespace: data-hub
  pool: caasp01-aba-pool
  userId: caasp01-aba
  userSecretName: ceph-secret-caasp01-aba

admin@jumphost:~/data-hub-build> kubectl apply -n data-hub -f ses-rbd-sc.yaml
admin@jumphost:~/data-hub-build> kubectl patch storageclass ses-rbd-sc -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"true"}}}'
admin@jumphost:~/data-hub-build> kubectl get storageclass
	Should show only one storage class and it is listed as (default)


