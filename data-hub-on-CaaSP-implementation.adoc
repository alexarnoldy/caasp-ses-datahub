This design is based on a very specific design. This design is intended for Proof of Concept, training and other non-production work. Various aspects of this design can be altered, however configuring *below* the minimum resource allocations for the cluster nodes greatly increases the risk of difficulty troubleshooting errors and failures, as well as the inability to satisfactorily complete the installation. 

.The success of the CaaSP and Data Hub installation relies highly on the diligence of the prepration work. Some important details that must be accomplished are:
* Configuration of a SES storage cluster that provides RBD and Rados Gateway services
** A minimum of 2TB of usable space
** The Rados Block Device service
*** Use https://ceph.com/pgcalc/ to get the commands to get the appropriate PG numbers for the pools
** Pool configured for Rados Gateway but RGW node will be configured as a Docker container
*** Use https://ceph.com/pgcalc/ to get the commands to create all RGW pools
*** https://www.suse.com/documentation/ses-4/book_storage_admin/data/ceph_rgw_manual.html
* DNS service configured with resolution for physical nodes as well as all CaaSP nodes
* Configuration of a TLS enabled Portus container registry (currently via docker-compose, running on the jumphost)
* Configuration of a CaaSPv3 cluster with the following minimums:
** Four physical hosts, each provisioned with 32 logical CPUs, 64GB of memory, and 128GB of locally attached disk space

** One jumphost VM, provisioned with access to 4 logical CPUs, 8GB of memory, and 80GB of locally attached disk space
** One admin VM node, provisioned with access to 16 logical CPUs, 16GB of memory, and 80GB of locally attached disk space
** Three master VM nodes, each provisioned with access to 16 logical CPUs, 24GB of memory, and 80GB of locally attached disk space
** Four worker VM nodes, each provisioned with access to 16 logical CPUs, 32GB of memory, and 256GB of locallly attached disk space

TIP: It is recommended that the master and worker nodes be spread across the available physical nodes with one of each on each physical node. The admin node and jumphost would then be run on the node that doesn't have a master node.

TIP: The virtual machine's locally attached disk space can be stored in SES RBD images. In this case, it is recommended that the disk files for the virtual machines assigned to each physical node be stored on a single RBD image. This would result in four RBD images created in the SES cluster, one for each physical node.

.It is recommended that the following suggestions be observed when installing the CaaSP cluster:
* Do not input the hostname of a node when assigning the node IP address. Only input the node name and domain name while providing the DNS information


.Rough outline of steps:
* Install physical nodes with SES
* Create an RBD image per host
* Map and mount RBD images
* Create VMs on RBD images
* Install Admin node through console
** Set up primary route to public router
** Secondary route to NAT router to the storage VLAN

Install cluster nodes with autoyast
	Set primary route to public router
	After installation, scp /etc/sysconfig/network/routes from admin to all nodes, then reboot all nodes

Verify that all nodes can ping google.com, admin.example.com, 172.16.200.130

ssh master1 mkdir -p /opt/docker-keepalived/
Create /opt/docker-keepalived/keepalived.conf on master1:
vrrp_instance VI_1 {
    state MASTER                # keepalived state
    interface eth0              # replace this with your interface
    virtual_router_id 40        
    priority 103
    track_interface {
        eth0                    # replace this with your interface
    }
    virtual_ipaddress {
        172.16.240.52           # replace this with your virtual IP
    }
    nopreempt
}

ssh master2 mkdir -p /opt/docker-keepalived/
Create /opt/docker-keepalived/keepalived.conf on master2:
vrrp_instance VI_1 {
    state BACKUP                # keepalived state
    interface eth0              # replace this with your interface
    virtual_router_id 40        
    priority 102
    track_interface {
        eth0                    # replace this with your interface
    }
    virtual_ipaddress {
        172.16.240.52           # replace this with your virtual IP
    }
    nopreempt
}

ssh master3 mkdir -p /opt/docker-keepalived/
Create /opt/docker-keepalived/keepalived.conf on master3:
vrrp_instance VI_1 {
    state BACKUP                # keepalived state
    interface eth0              # replace this with your interface
    virtual_router_id 40        
    priority 101
    track_interface {
        eth0                    # replace this with your interface
    }
    virtual_ipaddress {
        172.16.240.52           # replace this with your virtual IP
    }
    nopreempt
}

docker run -it -d --restart=always --net=host --privileged -v /opt/docker-keepalived/keepalived.conf:/etc/keepalived/keepalived.conf     --name haproxy-keepalived     pelin/haproxy-keepalived


Accept all nodes and form cluster. 
	Use master.example.com as API endpoint
	Use admin.example.com as cluster management endpoint

Wait for admin node to pickup new updates, then update admin and then rest of the cluster
	CMD: watch kubectl get nodes -o wide
		Best way to view progress of the update and determine if one node is causing problems


After nodes are all updated, start preparing the cluster for Data Hub:

Check the /etc/docker/daemon.json files before updating:
docker exec -it $(docker ps -q -f name="salt-master") salt -P 'roles:(admin|kube-master|kube-minion)' cmd.run "cat /etc/docker/daemon.json"

Each node (except for the admin) should have:
    {
      "registries": [
        {
          "Prefix": "https://registry.suse.com"
        },
        {
          "Prefix": "https://dhregistry.example.com:5000"
        }
      ],
      "iptables":false,
      "log-level": "warn",
      "log-driver": "json-file",
      "log-opts": {
        "max-size": "10m",
        "max-file": "5"
      }
    }

If not, need to copy that file to each node and restart docker


The pod that executes the SAP Data Hub Pipeline Engine API server must be able to access the internet while building the container images requested by pipeline operators:

docker exec -it $(docker ps -q -f name="salt-master") salt -P 'roles:(kube-master|kube-minion)' cmd.run "ping -c 2 google.com"


Jumphost (the Installation host) must have kubectl and helm installed. Both can be taken from the SUSE-CaaSP-3.0-Pool repository. Take info from admin node to add the repo to the jumphost.

sudo zypper in kubernetes-client
sudo zypper in helm

Copy the .kube/config file from the admin node to the jumphost. Change https://api.infra.caasp.local:6443 to https://master.example.com:6443
Copy all of the certificate files under /etc from admin node to jumphost
Add the following to the .kube/config file:
- context:
    cluster: default-cluster
    user: cluster-admin
    namespace: data-hub
  name: data-hub

Before you can start deployment, you must initialize Helm, the Kubernetes package manager, and provide it with the respective roles and permissions within the Kubernetes cluster.

kubectl create clusterrolebinding tiller --clusterrole=cluster-admin --serviceaccount=kube-system:tiller
helm init --client-only --service-account tiller

Create the data-hub namespace and use the data-hub configuration context:
kubectl create namespace data-hub
kubectl config use-context data-hub
kubectl config get-contexts


kubectl edit psp suse.caasp.psp.privileged
Search for allowedHostPaths first to ensure it isn’t already set elsewhere in the file
Add the following below and at the same indentation as “volumes:”

  allowedHostPaths:
  - pathPrefix: /

Create  clusterrolebinding.yaml:

apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: suse:caasp:psp:priviliged:default
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: suse:caasp:psp:privileged
subjects:
- kind: ServiceAccount
  name: default
  namespace: XXX
- kind: ServiceAccount
  name: vora-vsystem-XXX
  namespace: XXX
- kind: ServiceAccount
  name: XXX-elasticsearch
  namespace: XXX
- kind: ServiceAccount
  name: XXX-fluentd
  namespace: XXX
- kind: ServiceAccount
  name: XXX-nodeexporter
  namespace: XXX
- kind: ServiceAccount
  name: vora-vflow-server
  namespace: XXX


export NAMESPACE=data-hub && sed -i "s/XXX/${NAMESPACE}/g"  clusterrolebinding.yaml && kubectl apply -f clusterrolebinding.yaml

Copy the /etc/ceph files from the SES admin node to the CaaSP admin node:
admin:~ # scp 172.16.200.130:/etc/ceph/* /etc/ceph
Then, copy them from the CaaSP admin node to the rest of the CaaSP cluster:
admin:~ # for EE in 1 2 3 4; do scp /etc/ceph/* master$EE:/etc/ceph/; done
admin:~ # for EE in 1 2 3 4; do scp /etc/ceph/* worker$EE:/etc/ceph/; done

Verify all nodes can communicate with the CaaSP cluster:
docker exec -it $(docker ps -q -f name="salt-master") salt -P 'roles:(admin|kube-master|kube-minion)' cmd.run "ceph -s"


Set up Ceph secrets and default storage class in K8s:
Get the keys for data hub and admin users:
admin:~ # ceph auth ls  | egrep -A1 "caasp01|admin"

Encode keys (admin key as an example):
admin:~ # echo -n "AQCliWtcAAAAABAAMRgUejj5FCG/bvLBpmKDUw==" | base64
QVFDbGlXdGNBQUFBQUJBQU1SZ1Vlamo1RkNHL2J2TEJwbUtEVXc9PQ==

Create secrets:

admin@jumphost:~/data-hub-build> vi ceph-secret-admin.yaml
apiVersion: v1
kind: Secret
metadata:
  name: ceph-secret-admin
type: "kubernetes.io/rbd"
data:
  key: QVFDbGlXdGNBQUFBQUJBQU1SZ1Vlamo1RkNHL2J2TEJwbUtEVXc9PQ==

admin@jumphost:~/data-hub-build> vi ceph-secret-caasp01-aba.yaml
apiVersion: v1
kind: Secret
metadata:
  name: ceph-secret-caasp01-aba
type: "kubernetes.io/rbd"
data:
  key: QVFCT2JKdGN1OGFnTmhBQVkvb2RUWUFvZ3Q5T2g1WFptdDJDSEE9PQ==

admin@jumphost:~/data-hub-build> kubectl apply -n data-hub -f ceph-secret-caasp01-aba.yaml
admin@jumphost:~/data-hub-build> kubectl apply -n data-hub -f ceph-secret-admin.yaml


Create storage class and make it default:

admin@jumphost:~/data-hub-build> vi ses-rbd-sc.yaml
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: ses-rbd-sc
provisioner: kubernetes.io/rbd
parameters:
  monitors: 172.16.200.132:6789,172.16.200.133:6789,172.16.200.134:6789
  adminId: admin
  adminSecretName: ceph-secret-admin
  adminSecretNamespace: data-hub
  pool: caasp01-aba-pool
  userId: caasp01-aba
  userSecretName: ceph-secret-caasp01-aba

admin@jumphost:~/data-hub-build> kubectl apply -n data-hub -f ses-rbd-sc.yaml
admin@jumphost:~/data-hub-build> kubectl patch storageclass ses-rbd-sc -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"true"}}}'
admin@jumphost:~/data-hub-build> kubectl get storageclass
	Should show only one storage class and it is listed as (default)

Test that a PVC can be created and bound:

admin@jumphost:~/data-hub-build> vi test-pv-claim.yaml
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: test-pv-claim
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi

admin@jumphost:~/data-hub-build> kubectl apply -n data-hub -f test-pv-claim.yaml 
admin@jumphost:~/data-hub-build> kubectl get pvc
	After five to ten seconds, should show the PVC is bound
admin@jumphost:~/data-hub-build> kubectl delete -n data-hub -f test-pv-claim.yaml 

After deploying Portus, need to add it to Velum with its certificate (Need to include steps to deploy Portus)
Name: dhregistry.example.com
URL: https://dhregistry.example.com:5000
Certificate: (Copy in from the secrets directory in Portus)

Add imagePullSecret to default service account in the data-hub namespace:

admin:~ # kubectl create secret docker-registry dhregistry-secret -n data-hub --docker-server=dhregistry.example.com:5000 --docker-username=admin --docker-password=suse4SOC --docker-email=admin@example.com
admin:~ # kubectl patch sa default -n data-hub -p '"imagePullSecrets": [{"name": "dhregistry-secret" }]'

Test all nodes can push and pull to private registry (Must do the first docker login manually): 
docker exec -it $(docker ps -q -f name="salt-master") salt -P 'roles:(kube-master|kube-minion)' cmd.run "docker login dhregistry.example.com:5000 -u admin -p suse4SOC"

admin:~ # docker exec -it $(docker ps -q -f name="salt-master") salt -P 'roles:(kube-master|kube-minion)' cmd.run "hostname && docker pull nginx:latest && docker tag nginx:latest dhregistry.example.com:5000/nginx:latest && docker push dhregistry.example.com:5000/nginx:latest && docker pull dhregistry.example.com:5000/nginx:latest"


From the jumphost: 
	Add to /etc/ceph/rbdmap:
	caasp01-aba-vms/data-hub        id=admin,keyring=/etc/ceph/ceph.client.admin.keyring
	Add to /etc/fstab:
	/dev/rbd/caasp01-aba-vms/data-hub       /mnt/caasp01-aba-vms/data-hub   ext4    noauto  0  0
	Download the SAP Data Hub software from https://launchpad.support.sap.com/
		Save to /dev/rbd/caasp01-aba-vms/data-hub

If any master or worker nodes have less than 32GB, it is recommended to reboot each, in turn, before starting the installation to ensure they have the maximume amount of available memory for the installation.

admin@jumphost:~> cd /mnt/caasp01-aba-vms/data-hub/SAPDataHub-2.4.83-Foundation/; docker login dhregistry.example.com:5000 && 
./install.sh -e vora-cluster.components.dlog.replicationFactor="1" -e vora-cluster.components.dlog.standbyFactor="0" -e vora-context-deploy.secop.profile=notls  --image-pull-secret dhregistry-secret --pv-storage-class ses-rbd-sc --accept-license --namespace data-hub --registry dhregistry.example.com:5000 --enable-checkpoint-store no

	Add: --skip-preflight-checks if fails on helm version
	Use master.example.com as external Subject Alternative Name endpoint

After installation completes, it will provide important information for accessing Data Hub. I.e.:
############ Ports for external connectivity ############
# vora-tx-coordinator-ext/tc port:                  31450
# vora-tx-coordinator-ext/hana-wire port:           32692
# vora-textanalysis/textanalysis port:              32196
# vsystem/vsystem port:                             31273
#########################################################

#########################################################
# System Tenant created:    "system"
# System Tenant User:       "system"
# Initial Tenant created:   "default"
# Initial Tenant User:      "suse"
# User for tx-coordinator:  "default\suse"
#########################################################

Import the Portus root CA into Data Hub:

The root CA can be in .pem format (which is the same format but with a different suffix as .crt). It must be available on the system that is running the web browser used to access Data Hub.

The SAP Data Hub Launchpad will be available at https://master.example.com:31273
Log into the default Tenant as user suse and the password provided during installation.

Select Connection Management -> Import, select certificate file and select Open

If a node seems to be having problems, try draining it: kubectl drain <node> --delete-local-data --ignore-daemonsets
If the pods restart correctly, uncordon the node: kubectl uncordon <node>

Launch SAP HANA Express Docker container:
 
Host or VM must have lots of memory available (First deploy consumped about  9GB )

Add the following to /etc/sysctl.conf:
## HANA Express settings:
fs.file-max=20000000
fs.aio-max-nr=262144
vm.memory_failure_early_kill=1
vm.max_map_count=135217728
net.ipv4.ip_local_port_range=40000 60999

Must be logged into docker.io from system: docker login

Create /data/HANAExpress/passwd.json file:
{
  "master_password" : "suse4SOC"
}

sudo chown -R 12000:79 /data/HANAExpress
sudo chmod 600  /data/HANAExpress/passwd.json

docker pull store/saplabs/hanaexpress:2.00.036.00.20190223.1

sudo docker run -d -p 39013:39013 -p 39017:39017 -p 39041-39045:39041-39045 -p 1128-1129:1128-1129 -p 59013-59014:59013-59014 -v /data/HANAExpress:/hana/mounts --ulimit nofile=1048576:1048576 --sysctl kernel.shmmax=1073741824 --sysctl net.ipv4.ip_local_port_range='40000 60999' --sysctl kernel.shmmni=524288 --sysctl kernel.shmall=8388608 --name HXE store/saplabs/hanaexpress:2.00.036.00.20190223.1 --passwords-url file:///hana/mounts/passwd.json --agree-to-sap-license


##### Need to test pulling   dhregistry.example.com:5000/com.sap.hana.container/base-opensuse42.3-amd64   on nodes with smaller boot drives
// vim: set syntax=asciidoc:
